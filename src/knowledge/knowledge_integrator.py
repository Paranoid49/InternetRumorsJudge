import json
import logging
import time
from pathlib import Path
from datetime import datetime

# è®¾ç½®é¡¹ç›®è·¯å¾„ï¼ˆv0.9.0: ä½¿ç”¨ç»Ÿä¸€è·¯å¾„å·¥å…·ï¼‰
from src.utils.path_utils import setup_project_path, get_project_root
setup_project_path()

from src.retrievers.evidence_retriever import EvidenceKnowledgeBase
from src import config
from src.utils.llm_factory import create_dashscope_llm
from langchain_core.messages import HumanMessage, SystemMessage

# å»¶è¿Ÿå¯¼å…¥ç‰ˆæœ¬ç®¡ç†å™¨ï¼ˆé¿å…å¾ªç¯å¯¼å…¥ï¼‰
def _get_version_manager():
    from src.core.version_manager import VersionManager
    return VersionManager

# Setup logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')
logger = logging.getLogger("KnowledgeIntegrator")

# è·å–é¡¹ç›®æ ¹ç›®å½•ï¼ˆv0.9.0: ä½¿ç”¨ç»Ÿä¸€è·¯å¾„å·¥å…·ï¼‰
PROJECT_ROOT = get_project_root()

class KnowledgeIntegrator:
    def __init__(self,
                 reviewed_data_path: str = None,
                 rumor_data_dir: str = None,
                 model_name: str = "qwen3-max"):
        # é»˜è®¤è·¯å¾„ä½¿ç”¨é¡¹ç›®æ ¹ç›®å½•
        if reviewed_data_path is None:
            reviewed_data_path = str(PROJECT_ROOT / "data" / "reviewed" / "valid_corrections.json")
        if rumor_data_dir is None:
            rumor_data_dir = str(PROJECT_ROOT / "data" / "rumors")

        self.reviewed_data_path = Path(reviewed_data_path)
        self.rumor_data_dir = Path(rumor_data_dir)
        self.rumor_data_dir.mkdir(parents=True, exist_ok=True)
        
        # ä½¿ç”¨ç»Ÿä¸€çš„ LLM å·¥å‚ï¼ˆv0.9.0ï¼‰
        self.llm = create_dashscope_llm(
            model_name=model_name,
            temperature=0.7
        )

    def generate_knowledge_content(self, query: str, comment: str) -> str:
        """Use LLM to generate structured rumor knowledge file content."""
        prompt = f"""
ä½ æ˜¯ä¸€ä½ä¸“ä¸šçš„äº‹å®æ ¸æŸ¥å‘˜å’Œæ•°æ®æ•´ç†ä¸“å®¶ã€‚
è¯·æ ¹æ®ã€ç”¨æˆ·æŸ¥è¯¢ï¼ˆä¼ è¨€ï¼‰ã€‘å’Œã€ç”¨æˆ·åé¦ˆï¼ˆçº æ­£ï¼‰ã€‘ï¼Œç”Ÿæˆä¸€ä»½ç»“æ„åŒ–çš„è¾Ÿè°£çŸ¥è¯†æ¡£æ¡ˆã€‚
ä½ éœ€è¦åˆ©ç”¨ä½ çš„é€šç”¨çŸ¥è¯†æ¥éªŒè¯äº‹å®ï¼Œå¦‚æœç”¨æˆ·åé¦ˆæ¯”è¾ƒç®€ç•¥ï¼Œè¯·è¡¥å……ç›¸å…³ç»†èŠ‚ã€‚

ç”¨æˆ·æŸ¥è¯¢ï¼š{query}
ç”¨æˆ·åé¦ˆ/çº æ­£ï¼š{comment}

è¾“å‡ºæ ¼å¼ï¼ˆè¯·ä¸¥æ ¼éµå®ˆæ­¤æ¨¡æ¿ï¼‰ï¼š
æ ‡é¢˜ï¼šã€è¾Ÿè°£ã€‘å…³äºâ€œ<Title>â€çš„çœŸå®æƒ…å†µ
åˆ†ç±»ï¼š<Category>
çœŸå®æ€§ï¼š<True/False/Controversial>
å‘å¸ƒæ—¥æœŸï¼š<YYYY-MM-DD>
æ•°æ®ç¼–å·ï¼šAUTO-<Timestamp>
æ¥æºï¼š<Source, e.g. User Report, General Knowledge>
æ ‡ç­¾ï¼š<Tag1>, <Tag2>

ã€è°£è¨€å†…å®¹ã€‘
<å®Œæ•´çš„ä¼ è¨€é™ˆè¿°>

ã€çœŸç›¸æ ¸æŸ¥ã€‘
<è¯¦ç»†çš„äº‹å®æ ¸æŸ¥è§£é‡Š>

ã€å…³é”®äº‹å®ã€‘
â€¢ <äº‹å®ç‚¹ 1>
â€¢ <äº‹å®ç‚¹ 2>
â€¢ <äº‹å®ç‚¹ 3>

ã€ç»“è®ºã€‘
<ç»“è®ºé™ˆè¿°>

è¦æ±‚ï¼š
1. ç¡®ä¿å†…å®¹å‡†ç¡®å®¢è§‚ã€‚
2. â€œçœŸå®æ€§â€å­—æ®µåº”æ ¹æ®äº‹å®ç¡®å®šï¼ˆå¯¹äºè°£è¨€é€šå¸¸ä¸ºâ€œFalseâ€æˆ–â€œå‡â€ï¼‰ã€‚
3. æ‰€æœ‰å†…å®¹è¯·ä½¿ç”¨ä¸­æ–‡ã€‚
4. ä¸è¦è¾“å‡º Markdown ä»£ç å—ï¼Œä»…è¾“å‡ºçº¯æ–‡æœ¬å†…å®¹ã€‚
"""
        try:
            messages = [
                SystemMessage(content="ä½ æ˜¯ä¸€ä¸ªå¸®åŠ©ç”Ÿæˆç»“æ„åŒ–è¾Ÿè°£æ•°æ®çš„åŠ©æ‰‹ã€‚"),
                HumanMessage(content=prompt)
            ]
            response = self.llm.invoke(messages)
            return response.content.strip()
        except Exception as e:
            logger.error(f"LLM generation failed: {e}")
            return None

    def process_valid_feedback(self):
        """Process valid feedback and generate knowledge files."""
        if not self.reviewed_data_path.exists():
            logger.warning(f"No valid corrections file found at {self.reviewed_data_path}")
            return

        with open(self.reviewed_data_path, 'r', encoding='utf-8') as f:
            items = json.load(f)

        processed_count = 0
        new_items = []
        
        # Check if items already have 'integrated_at' field
        pending_items = [item for item in items if not item.get('integrated_at')]
        
        if not pending_items:
            logger.info("No new items to integrate.")
            return

        print(f"ğŸš€ Found {len(pending_items)} items to integrate...")

        for item in pending_items:
            query = item['query']
            comment = item['comment']
            
            logger.info(f"Processing: {query}")
            
            content = self.generate_knowledge_content(query, comment)
            if content:
                # Generate filename
                timestamp = int(time.time())
                safe_title = "".join([c for c in query if c.isalnum()])[:20]
                filename = f"AUTO_{timestamp}_{safe_title}.txt"
                file_path = self.rumor_data_dir / filename
                
                with open(file_path, 'w', encoding='utf-8') as f:
                    f.write(content)
                
                logger.info(f"âœ… Generated knowledge file: {filename}")
                
                item['integrated_at'] = datetime.now().isoformat()
                item['generated_file'] = filename
                processed_count += 1
            else:
                logger.error(f"âŒ Failed to generate content for: {query}")

            new_items.append(item)

        # Update the JSON file with processed status
        # We need to preserve the items that were already processed (not in pending_items)
        # But wait, 'items' loaded from file contains ALL items.
        # We iterated over 'pending_items' which are references to objects inside 'items' list (if generic python list behavior holds for json objects)
        # Yes, dictionaries are mutable.
        # So 'items' should be updated.
        
        with open(self.reviewed_data_path, 'w', encoding='utf-8') as f:
            json.dump(items, f, ensure_ascii=False, indent=2)

        if processed_count > 0:
            print(f"ğŸ‰ Successfully generated {processed_count} new knowledge files.")
            self.rebuild_knowledge_base()
        else:
            print("âš ï¸ No files generated.")

    def rebuild_knowledge_base(self):
        """
        Rebuild the vector knowledge base using double-buffering strategy.

        æ–°ç‰ˆæœ¬åœ¨åå°æ„å»ºï¼Œä¸ä¼šé˜»å¡å¹¶å‘æŸ¥è¯¢ã€‚æ„å»ºå®ŒæˆååŸå­æ€§åˆ‡æ¢ã€‚
        ä½¿ç”¨å¼ºåˆ¶å…¨é‡é‡å»ºï¼Œç¡®ä¿æ–°çŸ¥è¯†ç«‹å³ç”Ÿæ•ˆã€‚
        """
        print("ğŸ”„ Rebuilding Knowledge Base (using double-buffering strategy)...")
        logger.info("å¼€å§‹é‡æ„çŸ¥è¯†åº“ï¼ˆåŒç¼“å†²ç­–ç•¥ï¼Œçº¿ç¨‹å®‰å…¨ï¼‰")

        try:
            kb = EvidenceKnowledgeBase()

            # éªŒè¯ç‰ˆæœ¬ç®¡ç†å™¨å¯ç”¨ï¼ˆå¼ºåˆ¶è¦æ±‚ï¼‰
            if not kb._version_manager:
                raise RuntimeError("âŒ ç‰ˆæœ¬ç®¡ç†å™¨æœªåˆå§‹åŒ– - æ— æ³•ä¿è¯çº¿ç¨‹å®‰å…¨")

            # ä½¿ç”¨ç‰ˆæœ¬ç®¡ç†çš„åŒç¼“å†²æ„å»ºï¼ˆforce=Falseï¼Œä»ç„¶ä½¿ç”¨åŒç¼“å†²ï¼‰
            logger.info("ä½¿ç”¨ç‰ˆæœ¬ç®¡ç†çš„åŒç¼“å†²æ„å»ºï¼Œä¸ä¼šé˜»å¡å¹¶å‘æŸ¥è¯¢")
            kb.build(force=False, incremental=False)  # å…¨é‡é‡å»ºæ–°ç‰ˆæœ¬ï¼ˆåŒç¼“å†²ï¼‰
            print("âœ… Knowledge Base rebuilt successfully with versioning!")

        except Exception as e:
            logger.error(f"Failed to rebuild Knowledge Base: {e}", exc_info=True)
            print(f"âŒ Failed to rebuild Knowledge Base: {e}")

if __name__ == "__main__":
    integrator = KnowledgeIntegrator()
    integrator.process_valid_feedback()
