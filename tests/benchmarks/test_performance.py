"""
ç³»ç»Ÿæ€§èƒ½åŸºå‡†æµ‹è¯•

æµ‹è¯•ä¸åŒåœºæ™¯ä¸‹çš„å“åº”æ—¶é—´ï¼Œç»™å‡ºé‡åŒ–ç»“æœ
"""
import json
import logging
import sys
import time
from pathlib import Path
from typing import List, Dict, Any
from datetime import datetime

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ° Python è·¯å¾„
sys.path.insert(0, str(Path(__file__).parent.parent))

from src.core.pipeline import RumorJudgeEngine

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.WARNING, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger("PerformanceBenchmark")


class PerformanceBenchmark:
    """æ€§èƒ½åŸºå‡†æµ‹è¯•"""

    def __init__(self):
        self.engine = RumorJudgeEngine()
        self.results = []

    def test_cache_hit_exact(self, query: str, iterations: int = 5) -> Dict:
        """
        åœºæ™¯1: ç²¾ç¡®ç¼“å­˜å‘½ä¸­

        é¢„æœŸå“åº”æ—¶é—´: < 5ms
        """
        print(f"\n[åœºæ™¯1] ç²¾ç¡®ç¼“å­˜å‘½ä¸­æµ‹è¯• ({iterations}æ¬¡è¿­ä»£)")
        print(f"æŸ¥è¯¢: {query}")

        times = []
        for i in range(iterations):
            start = time.time()
            result = self.engine.run(query, use_cache=True)
            duration = (time.time() - start) * 1000  # è½¬æ¢ä¸ºæ¯«ç§’
            times.append(duration)

            # ç¬¬ä¸€æ¬¡å¯èƒ½æœªå‘½ä¸­ç¼“å­˜ï¼Œä»ç¬¬äºŒæ¬¡å¼€å§‹ç»Ÿè®¡
            if i == 0:
                print(f"  ç¬¬1æ¬¡æŸ¥è¯¢: {duration:.2f}ms (ç¼“å­˜æœªå‘½ä¸­: {result.is_cached})")
            else:
                print(f"  ç¬¬{i+1}æ¬¡æŸ¥è¯¢: {duration:.2f}ms (ç¼“å­˜: {result.is_cached})")

        # æ’é™¤ç¬¬ä¸€æ¬¡ï¼ˆå¯èƒ½æœªå‘½ä¸­ç¼“å­˜ï¼‰
        cache_hits = [t for i, t in enumerate(times) if i > 0]

        return {
            "scenario": "ç²¾ç¡®ç¼“å­˜å‘½ä¸­",
            "query": query,
            "iterations": len(cache_hits),
            "min_ms": round(min(cache_hits), 2),
            "max_ms": round(max(cache_hits), 2),
            "avg_ms": round(sum(cache_hits) / len(cache_hits), 2),
            "median_ms": round(sorted(cache_hits)[len(cache_hits)//2], 2),
            "p95_ms": round(sorted(cache_hits)[int(len(cache_hits)*0.95)], 2),
            "p99_ms": round(sorted(cache_hits)[int(len(cache_hits)*0.99)], 2),
            "times": cache_hits
        }

    def test_cache_hit_semantic(self, query: str, iterations: int = 5) -> Dict:
        """
        åœºæ™¯2: è¯­ä¹‰ç¼“å­˜å‘½ä¸­

        é¢„æœŸå“åº”æ—¶é—´: < 50ms
        """
        print(f"\n[åœºæ™¯2] è¯­ä¹‰ç¼“å­˜å‘½ä¸­æµ‹è¯• ({iterations}æ¬¡è¿­ä»£)")
        print(f"æŸ¥è¯¢: {query}")

        # é¦–å…ˆæŸ¥è¯¢ç›¸ä¼¼çš„é—®é¢˜å»ºç«‹ç¼“å­˜
        similar_query = query.replace("ï¼Ÿ", "").replace("å—", "").strip() + "æ˜¯çœŸçš„å—"
        print(f"  é¢„å…ˆæŸ¥è¯¢ç›¸ä¼¼é—®é¢˜: {similar_query}")
        self.engine.run(similar_query, use_cache=True)
        time.sleep(0.5)

        times = []
        for i in range(iterations):
            start = time.time()
            result = self.engine.run(query, use_cache=True)
            duration = (time.time() - start) * 1000
            times.append(duration)
            print(f"  ç¬¬{i+1}æ¬¡æŸ¥è¯¢: {duration:.2f}ms (ç¼“å­˜: {result.is_cached})")

        return {
            "scenario": "è¯­ä¹‰ç¼“å­˜å‘½ä¸­",
            "query": query,
            "iterations": iterations,
            "min_ms": round(min(times), 2),
            "max_ms": round(max(times), 2),
            "avg_ms": round(sum(times) / len(times), 2),
            "median_ms": round(sorted(times)[len(times)//2], 2),
            "p95_ms": round(sorted(times)[int(len(times)*0.95)], 2),
            "p99_ms": round(sorted(times)[int(len(times)*0.99)], 2),
            "times": times
        }

    def test_local_rag_only(self, query: str, iterations: int = 3) -> Dict:
        """
        åœºæ™¯3: æœ¬åœ°RAGï¼ˆæ— ç½‘ç»œæœç´¢ï¼‰

        é¢„æœŸå“åº”æ—¶é—´: 5-8ç§’
        """
        print(f"\n[åœºæ™¯3] æœ¬åœ°RAGæµ‹è¯• ({iterations}æ¬¡è¿­ä»£)")
        print(f"æŸ¥è¯¢: {query}")

        times = []
        for i in range(iterations):
            start = time.time()
            result = self.engine.run(query, use_cache=False)  # ç¦ç”¨ç¼“å­˜
            duration = time.time() - start
            times.append(duration)

            web_search = "æ˜¯" if result.is_web_search else "å¦"
            print(f"  ç¬¬{i+1}æ¬¡æŸ¥è¯¢: {duration:.2f}s (è”ç½‘: {web_search}, è¯æ®: {len(result.retrieved_evidence)}æ¡)")

        return {
            "scenario": "æœ¬åœ°RAG",
            "query": query,
            "iterations": iterations,
            "min_s": round(min(times), 2),
            "max_s": round(max(times), 2),
            "avg_s": round(sum(times) / len(times), 2),
            "median_s": round(sorted(times)[len(times)//2], 2),
            "p95_s": round(sorted(times)[int(len(times)*0.95)], 2),
            "p99_s": round(sorted(times)[int(len(times)*0.99)], 2),
            "times": times
        }

    def test_full_pipeline(self, query: str, iterations: int = 2) -> Dict:
        """
        åœºæ™¯4: å®Œæ•´æµç¨‹ï¼ˆåŒ…å«ç½‘ç»œæœç´¢ï¼‰

        é¢„æœŸå“åº”æ—¶é—´: 20-30ç§’
        """
        print(f"\n[åœºæ™¯4] å®Œæ•´æµç¨‹æµ‹è¯•ï¼ˆå«ç½‘ç»œæœç´¢ï¼‰({iterations}æ¬¡è¿­ä»£)")
        print(f"æŸ¥è¯¢: {query}")

        times = []
        for i in range(iterations):
            start = time.time()
            result = self.engine.run(query, use_cache=False)
            duration = time.time() - start
            times.append(duration)

            web_search = "æ˜¯" if result.is_web_search else "å¦"
            print(f"  ç¬¬{i+1}æ¬¡æŸ¥è¯¢: {duration:.2f}s (è”ç½‘: {web_search}, è£å†³: {result.final_verdict})")

        return {
            "scenario": "å®Œæ•´æµç¨‹",
            "query": query,
            "iterations": iterations,
            "min_s": round(min(times), 2),
            "max_s": round(max(times), 2),
            "avg_s": round(sum(times) / len(times), 2),
            "median_s": round(sorted(times)[len(times)//2], 2),
            "p95_s": round(sorted(times)[int(len(times)*0.95)], 2),
            "p99_s": round(sorted(times)[int(len(times)*0.99)], 2),
            "times": times
        }

    def test_batch_queries(self, queries: List[str]) -> Dict:
        """
        åœºæ™¯5: æ‰¹é‡æŸ¥è¯¢ï¼ˆæ¨¡æ‹Ÿå¹¶å‘åœºæ™¯ï¼‰

        æµ‹è¯•ç³»ç»Ÿåœ¨è¿ç»­å¤šä¸ªæŸ¥è¯¢ä¸‹çš„æ€§èƒ½è¡¨ç°
        """
        print(f"\n[åœºæ™¯5] æ‰¹é‡æŸ¥è¯¢æµ‹è¯• ({len(queries)}ä¸ªæŸ¥è¯¢)")

        start_all = time.time()
        individual_times = []

        for i, query in enumerate(queries):
            start = time.time()
            result = self.engine.run(query, use_cache=True)  # å…è®¸ç¼“å­˜
            duration = time.time() - start
            individual_times.append(duration)

            cached = "[ç¼“å­˜]" if result.is_cached else "[å®æ—¶]"
            print(f"  æŸ¥è¯¢{i+1}: {duration:.2f}s {cached} - {result.final_verdict}")

        total_time = time.time() - start_all

        return {
            "scenario": "æ‰¹é‡æŸ¥è¯¢",
            "total_queries": len(queries),
            "total_time_s": round(total_time, 2),
            "avg_time_s": round(sum(individual_times) / len(individual_times), 2),
            "min_s": round(min(individual_times), 2),
            "max_s": round(max(individual_times), 2),
            "cache_hit_rate": round(sum(1 for i, q in enumerate(queries) if i > 0 and individual_times[i] < 1) / (len(queries) - 1), 2),
            "individual_times": individual_times
        }

    def run_all_benchmarks(self) -> Dict:
        """è¿è¡Œæ‰€æœ‰åŸºå‡†æµ‹è¯•"""
        print("="*70)
        print("ç³»ç»Ÿæ€§èƒ½åŸºå‡†æµ‹è¯•")
        print("="*70)
        print(f"å¼€å§‹æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")

        # æµ‹è¯•æŸ¥è¯¢
        test_queries = {
            "cache_exact": "ç»´ç”Ÿç´ Cèƒ½é¢„é˜²æ„Ÿå†’å—ï¼Ÿ",
            "cache_semantic": "åƒç»´ç”Ÿç´ Cä¼šè‡´ç™Œå—",
            "local_rag": "å–éš”å¤œæ°´ä¼šè‡´ç™Œå—",  # æœ¬åœ°åº“æœ‰æ­¤å†…å®¹
            "full_pipeline": "2025å¹´åœ°çƒä¼šæ¯ç­å—",  # éœ€è¦è”ç½‘æœç´¢
            "batch": [
                "ç»´ç”Ÿç´ Cèƒ½é¢„é˜²æ„Ÿå†’å—ï¼Ÿ",
                "å–éš”å¤œæ°´ä¼šè‡´ç™Œå—",
                "åƒæ´‹è‘±èƒ½æ²»æ„Ÿå†’å—",
                "æ‰‹æœºè¾å°„ä¼šå¯¼è‡´è„‘ç™Œå—",
                "å¸çƒŸæœ‰å®³å¥åº·å—"
            ]
        }

        results = {}

        # åœºæ™¯1: ç²¾ç¡®ç¼“å­˜å‘½ä¸­
        results["cache_exact"] = self.test_cache_hit_exact(
            test_queries["cache_exact"],
            iterations=5
        )

        # åœºæ™¯2: è¯­ä¹‰ç¼“å­˜å‘½ä¸­
        results["cache_semantic"] = self.test_cache_hit_semantic(
            test_queries["cache_semantic"],
            iterations=5
        )

        # åœºæ™¯3: æœ¬åœ°RAG
        results["local_rag"] = self.test_local_rag_only(
            test_queries["local_rag"],
            iterations=3
        )

        # åœºæ™¯4: å®Œæ•´æµç¨‹
        results["full_pipeline"] = self.test_full_pipeline(
            test_queries["full_pipeline"],
            iterations=2
        )

        # åœºæ™¯5: æ‰¹é‡æŸ¥è¯¢
        results["batch"] = self.test_batch_queries(test_queries["batch"])

        # ç”ŸæˆæŠ¥å‘Š
        return {
            "timestamp": datetime.now().isoformat(),
            "system_info": {
                "python_version": sys.version,
                "platform": sys.platform
            },
            "benchmarks": results
        }

    def print_report(self, report: Dict):
        """æ‰“å°æ€§èƒ½æµ‹è¯•æŠ¥å‘Š"""
        print("\n" + "="*70)
        print("æ€§èƒ½æµ‹è¯•æŠ¥å‘Š")
        print("="*70)

        benchmarks = report["benchmarks"]

        for key, data in benchmarks.items():
            print(f"\nã€{data['scenario']}ã€‘")
            print(f"  æµ‹è¯•æŸ¥è¯¢: {data.get('query', 'N/A')}")

            if 'avg_ms' in data:  # æ¯«ç§’çº§ç»“æœ
                print(f"  è¿­ä»£æ¬¡æ•°: {data['iterations']}")
                print(f"  å¹³å‡å“åº”: {data['avg_ms']} ms")
                print(f"  ä¸­ä½æ•°: {data['median_ms']} ms")
                print(f"  æœ€å°å€¼: {data['min_ms']} ms")
                print(f"  æœ€å¤§å€¼: {data['max_ms']} ms")
                print(f"  P95: {data['p95_ms']} ms")
                print(f"  P99: {data['p99_ms']} ms")
            elif 'avg_s' in data:  # ç§’çº§ç»“æœ
                print(f"  è¿­ä»£æ¬¡æ•°: {data['iterations']}")
                print(f"  å¹³å‡å“åº”: {data['avg_s']} s")
                print(f"  ä¸­ä½æ•°: {data['median_s']} s")
                print(f"  æœ€å°å€¼: {data['min_s']} s")
                print(f"  æœ€å¤§å€¼: {data['max_s']} s")
                print(f"  P95: {data['p95_s']} s")
                print(f"  P99: {data['p99_s']} s")
            elif 'avg_time_s' in data:  # æ‰¹é‡æŸ¥è¯¢
                print(f"  æŸ¥è¯¢æ•°é‡: {data['total_queries']}")
                print(f"  æ€»è€—æ—¶: {data['total_time_s']} s")
                print(f"  å¹³å‡è€—æ—¶: {data['avg_time_s']} s")
                print(f"  ç¼“å­˜å‘½ä¸­ç‡: {data['cache_hit_rate']*100:.1f}%")

        # æ€»ç»“
        print("\n" + "="*70)
        print("å…³é”®æ€§èƒ½æŒ‡æ ‡æ€»ç»“")
        print("="*70)

        cache_exact = benchmarks["cache_exact"]["avg_ms"]
        local_rag = benchmarks["local_rag"]["avg_s"]
        full_pipeline = benchmarks["full_pipeline"]["avg_s"]

        print(f"âœ… ç²¾ç¡®ç¼“å­˜å“åº”: {cache_exact} ms (ç›®æ ‡: < 5ms)")
        print(f"âœ… æœ¬åœ°RAGå“åº”: {local_rag} s (ç›®æ ‡: < 8s)")
        print(f"âœ… å®Œæ•´æµç¨‹å“åº”: {full_pipeline} s (ç›®æ ‡: < 30s)")
        print(f"âœ… æ‰¹é‡æŸ¥è¯¢ç¼“å­˜å‘½ä¸­ç‡: {benchmarks['batch']['cache_hit_rate']*100:.1f}%")

        # æ€§èƒ½è¯„çº§
        if cache_exact < 5:
            print(f"\nğŸ† æ€§èƒ½è¯„çº§: ä¼˜ç§€ (ç¼“å­˜å“åº”æå¿«)")
        elif cache_exact < 50:
            print(f"\nğŸ¥ˆ æ€§èƒ½è¯„çº§: è‰¯å¥½ (ç¼“å­˜å“åº”æ­£å¸¸)")
        else:
            print(f"\nâš ï¸  æ€§èƒ½è¯„çº§: éœ€ä¼˜åŒ– (ç¼“å­˜å“åº”åæ…¢)")

    def save_report(self, report: Dict):
        """ä¿å­˜æ€§èƒ½æµ‹è¯•æŠ¥å‘Š"""
        report_file = Path(__file__).parent.parent / "performance_report.json"
        with open(report_file, 'w', encoding='utf-8') as f:
            json.dump(report, f, indent=2, ensure_ascii=False)
        print(f"\næŠ¥å‘Šå·²ä¿å­˜è‡³: {report_file}")


def main():
    """ä¸»å‡½æ•°"""
    try:
        benchmark = PerformanceBenchmark()
        report = benchmark.run_all_benchmarks()
        benchmark.print_report(report)
        benchmark.save_report(report)
        return 0
    except Exception as e:
        logger.error(f"æ€§èƒ½æµ‹è¯•å¤±è´¥: {e}", exc_info=True)
        return 1


if __name__ == "__main__":
    sys.exit(main())
