# evaluation.py
import json
from typing import Dict, List
from modern_main import ModernRumorVerificationSystem  # ä½ çš„ä¸»ç³»ç»Ÿ

class RumourEvaluator:
    def __init__(self, system):
        self.system = system
        # è¯„ä¼°æ•°æ®é›† (åŸºäºä½ ä¹‹å‰çš„10æ¡æ•°æ®ï¼Œè¿™é‡Œä»¥2æ¡ä¸ºä¾‹)
        self.test_dataset = [
            {
                "input": "åƒè”æåå¼€è½¦ä¼šè¢«æŸ¥å‡ºé…’é©¾",
                "expected_truth": "å‡",  # æœŸæœ›çš„æœ€ç»ˆç»“è®º
                "expected_entities": ["è”æ", "é…’é©¾"],  # æœŸæœ›è¯†åˆ«å‡ºçš„å…³é”®å®ä½“
                "category": "ç”Ÿæ´»å¸¸è¯†"
            },
            {
                "input": "é…¸æ€§ä½“è´¨æ˜¯ç™¾ç—…ä¹‹æºï¼Œå¤šåƒç¢±æ€§é£Ÿç‰©å¯ä»¥æŠ—ç™Œ",
                "expected_truth": "å‡",
                "expected_entities": ["é…¸æ€§ä½“è´¨", "ç¢±æ€§é£Ÿç‰©", "æŠ—ç™Œ"],
                "category": "å¥åº·å…»ç”Ÿ"
            }
            # ... åŠ å…¥ä½ çš„å…¨éƒ¨10æ¡æ•°æ®
        ]
    
    def evaluate_parsing(self, result: Dict, test_case: Dict) -> float:
        """è¯„ä¼°æŸ¥è¯¢è§£æçš„å‡†ç¡®æ€§"""
        score = 0
        parsed = result.get("parsed_query", {})
        
        # æ£€æŸ¥æ˜¯å¦è¯†åˆ«å‡ºäº†æ ¸å¿ƒå®ä½“
        detected_entities = parsed.get("entity", "")
        expected_entities = test_case["expected_entities"]
        for exp_entity in expected_entities:
            if exp_entity in detected_entities:
                score += 1
                break
        
        # æ£€æŸ¥åˆ†ç±»æ˜¯å¦æ­£ç¡®ï¼ˆå¦‚æœç³»ç»Ÿæœ‰è¾“å‡ºåˆ†ç±»ï¼‰
        if parsed.get("category") == test_case.get("category"):
            score += 1
            
        return score / 2  # å½’ä¸€åŒ–åˆ°0-1
    
    def evaluate_retrieval(self, result: Dict) -> float:
        """è¯„ä¼°è¯æ®æ£€ç´¢çš„ç›¸å…³æ€§"""
        evidence_list = result.get("evidence", [])
        if not evidence_list:
            return 0.0
        
        # ç®€å•è¯„ä¼°ï¼šæ˜¯å¦æœ‰è¯æ®è¿”å›ï¼Ÿè¯æ®æ•°é‡æ˜¯å¦åˆç†ï¼Ÿ
        # æ›´é«˜çº§çš„åšæ³•ï¼šå¯ä»¥ç”¨æ¨¡å‹è¯„ä¼°è¯æ®ä¸æŸ¥è¯¢çš„ç›¸å…³æ€§
        has_evidence = len(evidence_list) > 0
        reasonable_count = 1 <= len(evidence_list) <= 5
        
        return 1.0 if (has_evidence and reasonable_count) else 0.5
    
    def evaluate_judgment(self, final_report: str, test_case: Dict) -> float:
        """è¯„ä¼°æœ€ç»ˆç»“è®ºçš„æ­£ç¡®æ€§ï¼ˆç®€åŒ–ç‰ˆï¼‰"""
        expected = test_case["expected_truth"]
        
        # åœ¨æœ€ç»ˆæŠ¥å‘Šä¸­æŸ¥æ‰¾ç»“è®ºå…³é”®è¯
        report_lower = final_report.lower()
        if expected == "å‡":
            if "å‡" in report_lower or "ä¸å®" in report_lower or "é”™è¯¯" in report_lower:
                return 1.0
        elif expected == "çœŸ":
            if "çœŸ" in report_lower or "æ­£ç¡®" in report_lower or "å±å®" in report_lower:
                return 1.0
        # å¯¹äºâ€œè¯æ®ä¸è¶³â€ç­‰æƒ…å†µå¯ä»¥æ‰©å±•
        
        return 0.0
    
    def run_full_evaluation(self) -> Dict:
        """è¿è¡Œå®Œæ•´è¯„ä¼°"""
        print("å¼€å§‹å…¨é¢è¯„ä¼°ç³»ç»Ÿæ€§èƒ½...")
        print("=" * 60)
        
        total_scores = {"parsing": 0, "retrieval": 0, "judgment": 0}
        
        for i, test_case in enumerate(self.test_dataset):
            print(f"\nğŸ”¬ æµ‹è¯•æ¡ˆä¾‹ {i+1}: {test_case['input']}")
            
            try:
                # è¿è¡Œç³»ç»Ÿ
                result = self.system.verify(test_case["input"])
                
                # è¯„ä¼°å„ç¯èŠ‚
                parse_score = self.evaluate_parsing(result, test_case)
                retrieval_score = self.evaluate_retrieval(result)
                judgment_score = self.evaluate_judgment(result.get("final_report", ""), test_case)
                
                print(f"  è§£æè¯„åˆ†: {parse_score:.2f}")
                print(f"  æ£€ç´¢è¯„åˆ†: {retrieval_score:.2f}")
                print(f"  ç»“è®ºè¯„åˆ†: {judgment_score:.2f}")
                
                # ç´¯åŠ åˆ†æ•°
                total_scores["parsing"] += parse_score
                total_scores["retrieval"] += retrieval_score
                total_scores["judgment"] += judgment_score
                
            except Exception as e:
                print(f"  æµ‹è¯•å¤±è´¥: {e}")
                continue
        
        # è®¡ç®—å¹³å‡åˆ†
        n = len(self.test_dataset)
        avg_scores = {k: v/n for k, v in total_scores.items()}
        avg_scores["overall"] = sum(avg_scores.values()) / len(avg_scores)
        
        print("\n" + "=" * 60)
        print("ğŸ“Š è¯„ä¼°ç»“æœæ±‡æ€»:")
        for key, score in avg_scores.items():
            print(f"  {key}: {score:.2%}")
        
        # ç”Ÿæˆæ”¹è¿›å»ºè®®
        self.generate_recommendations(avg_scores)
        
        return avg_scores
    
    def generate_recommendations(self, scores: Dict):
        """æ ¹æ®è¯„åˆ†ç”Ÿæˆæ”¹è¿›å»ºè®®"""
        print("\nğŸ’¡ ä¼˜åŒ–å»ºè®®:")
        
        if scores["parsing"] < 0.8:
            print("  â€¢ ä¼˜åŒ–æŸ¥è¯¢è§£ææ™ºèƒ½ä½“çš„æç¤ºè¯ï¼Œç¡®ä¿å‡†ç¡®æå–å®ä½“å’Œä¸»å¼ ")
        
        if scores["retrieval"] < 0.7:
            print("  â€¢ æ£€æŸ¥å‘é‡åº“è´¨é‡ï¼Œå¯èƒ½éœ€è¦ï¼š")
            print("    - å¢åŠ æ›´å¤šæ ·åŒ–çš„è¾Ÿè°£æ•°æ®")
            print("    - è°ƒæ•´æ–‡æœ¬åˆ†å‰²ç­–ç•¥ï¼ˆchunk_sizeï¼‰")
            print("    - å°è¯•ä¸åŒçš„åµŒå…¥æ¨¡å‹")
        
        if scores["judgment"] < 0.6:
            print("  â€¢ é‡ç‚¹ä¼˜åŒ–åˆ†æä¸è£å†³æ™ºèƒ½ä½“ï¼š")
            print("    - åœ¨æç¤ºè¯ä¸­åŠ å…¥æ›´æ˜ç¡®çš„åˆ¤æ–­æ ‡å‡†")
            print("    - å¤„ç†'éƒ¨åˆ†çœŸå®'ã€'è¯æ®çŸ›ç›¾'ç­‰è¾¹ç¼˜æƒ…å†µ")
            print("    - è®©ç»“è®ºä¸è¯æ®çš„å…³è”æ›´æ˜¾å¼")

if __name__ == "__main__":
    # åˆå§‹åŒ–ä½ çš„ç³»ç»Ÿ
    system = ModernRumorVerificationSystem()
    evaluator = RumourEvaluator(system)
    
    # è¿è¡Œè¯„ä¼°
    results = evaluator.run_full_evaluation()
    
    # ä¿å­˜è¯„ä¼°ç»“æœ
    with open("evaluation_results.json", "w", encoding="utf-8") as f:
        json.dump({
            "timestamp": "2024-01-01",  # å®é™…ä½¿ç”¨æ—¶æ›¿æ¢ä¸ºdatetime.now()
            "test_cases_count": len(evaluator.test_dataset),
            "scores": results
        }, f, ensure_ascii=False, indent=2)