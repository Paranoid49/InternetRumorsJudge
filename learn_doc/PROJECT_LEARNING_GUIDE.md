# äº’è”ç½‘è°£è¨€ç²‰ç¢æœº - é¡¹ç›®å­¦ä¹ æŒ‡å—

> åŸºäºRAGå’ŒLLMçš„æ™ºèƒ½è°£è¨€æ ¸æŸ¥ç³»ç»Ÿå­¦ä¹ æ–‡æ¡£
>
> ç”Ÿæˆæ—¶é—´: 2026-02-09
> é¡¹ç›®ç‰ˆæœ¬: v0.7.0

---

## ç›®å½•

- [ç¬¬ä¸€éƒ¨åˆ†ï¼šé¡¹ç›®æ¦‚è§ˆ](#ç¬¬ä¸€éƒ¨åˆ†é¡¹ç›®æ¦‚è§ˆ)
  - [1.1 é¡¹ç›®ç®€ä»‹](#11-é¡¹ç›®ç®€ä»‹)
  - [1.2 æ•´ä½“æ¶æ„](#12-æ•´ä½“æ¶æ„)
  - [1.3 ä¸»è¦å·¥ä½œæµç¨‹](#13-ä¸»è¦å·¥ä½œæµç¨‹)
  - [1.4 æŠ€æœ¯æ ˆ](#14-æŠ€æœ¯æ ˆ)
- [ç¬¬äºŒéƒ¨åˆ†ï¼šæ ¸å¿ƒæ¨¡å—è¯¦è§£](#ç¬¬äºŒéƒ¨åˆ†æ ¸å¿ƒæ¨¡å—è¯¦è§£)
  - [2.1 å¼•æ“å±‚ - RumorJudgeEngine](#21-å¼•æ“å±‚---rumorjudgeengine)
  - [2.2 åè°ƒå™¨æ¨¡å¼](#22-åè°ƒå™¨æ¨¡å¼)
  - [2.3 ç¼“å­˜ç®¡ç†ç³»ç»Ÿ](#23-ç¼“å­˜ç®¡ç†ç³»ç»Ÿ)
  - [2.4 æ··åˆæ£€ç´¢ç³»ç»Ÿ](#24-æ··åˆæ£€ç´¢ç³»ç»Ÿ)
  - [2.5 è¯æ®åˆ†æç³»ç»Ÿ](#25-è¯æ®åˆ†æç³»ç»Ÿ)
- [ç¬¬ä¸‰éƒ¨åˆ†ï¼šåŸºç¡€è®¾æ–½](#ç¬¬ä¸‰éƒ¨åˆ†åŸºç¡€è®¾æ–½)
  - [3.1 çº¿ç¨‹å®‰å…¨æœºåˆ¶](#31-çº¿ç¨‹å®‰å…¨æœºåˆ¶)
  - [3.2 ç‰ˆæœ¬ç®¡ç†ç³»ç»Ÿ](#32-ç‰ˆæœ¬ç®¡ç†ç³»ç»Ÿ)
  - [3.3 APIç›‘æ§ç³»ç»Ÿ](#33-apiç›‘æ§ç³»ç»Ÿ)
  - [3.4 åŠ¨æ€å¹¶è¡Œåº¦é…ç½®](#34-åŠ¨æ€å¹¶è¡Œåº¦é…ç½®)
- [ç¬¬å››éƒ¨åˆ†ï¼šè®¾è®¡æ¨¡å¼ä¸æœ€ä½³å®è·µ](#ç¬¬å››éƒ¨åˆ†è®¾è®¡æ¨¡å¼ä¸æœ€ä½³å®è·µ)
- [ç¬¬äº”éƒ¨åˆ†ï¼šå¼€å‘æŒ‡å—](#ç¬¬äº”éƒ¨åˆ†å¼€å‘æŒ‡å—)

---

## ç¬¬ä¸€éƒ¨åˆ†ï¼šé¡¹ç›®æ¦‚è§ˆ

### 1.1 é¡¹ç›®ç®€ä»‹

**é¡¹ç›®ç›®æ ‡ï¼š** æ„å»ºä¸€ä¸ªæ™ºèƒ½çš„äº’è”ç½‘è°£è¨€æ ¸æŸ¥ç³»ç»Ÿï¼Œèƒ½å¤Ÿè‡ªåŠ¨åˆ†æç”¨æˆ·è¾“å…¥çš„å¯ç–‘ä¿¡æ¯ï¼Œé€šè¿‡æ£€ç´¢æƒå¨è¯æ®å¹¶è¿›è¡Œå¤šè§’åº¦åˆ†æï¼Œç»™å‡ºå¯ä¿¡çš„çœŸä¼ªè£å†³ã€‚

**æ ¸å¿ƒç‰¹æ€§ï¼š**

- ğŸ” **æ™ºèƒ½æ ¸æŸ¥**ï¼šåŸºäºRAGçš„è¯­ä¹‰æ£€ç´¢ï¼Œå‡†ç¡®è¯„ä¼°è°£è¨€çœŸä¼ª
- ğŸŒ **å®æ—¶æœç´¢**ï¼šé›†æˆäº’è”ç½‘æœç´¢ï¼Œè·å–æœ€æ–°ä¿¡æ¯
- ğŸ§  **å¤šæ¨¡å‹åä½œ**ï¼šè§£æã€åˆ†æã€è£å†³ä½¿ç”¨ä¸åŒLLMæ¨¡å‹
- âš¡ **é«˜æ€§èƒ½**ï¼šå¹¶è¡Œå¤„ç†ã€åŠ¨æ€å¹¶è¡Œåº¦è°ƒæ•´ã€è¯­ä¹‰ç¼“å­˜
- ğŸ“Š **APIç›‘æ§**ï¼šå®æ—¶è¿½è¸ªAPIä½¿ç”¨å’Œæˆæœ¬
- ğŸ”„ **è‡ªæˆ‘è¿›åŒ–**ï¼šè‡ªåŠ¨å°†é«˜ç½®ä¿¡åº¦ç»“æœè½¬åŒ–ä¸ºæœ¬åœ°çŸ¥è¯†
- ğŸ›¡ï¸ **çº¿ç¨‹å®‰å…¨**ï¼šå®Œæ•´çš„å¹¶å‘å®‰å…¨ä¿æŠ¤

### 1.2 æ•´ä½“æ¶æ„

```mermaid
graph TB
    subgraph "ç”¨æˆ·æ¥å£å±‚"
        A[å‘½ä»¤è¡Œæ¥å£ scripts/main.py]
        B[Webæ¥å£ src/services/web_interface.py]
        C[APIæœåŠ¡ src/services/api_service.py]
    end

    subgraph "å¼•æ“å±‚"
        D[RumorJudgeEngine<br/>å•ä¾‹æ¨¡å¼<br/>src/core/pipeline.py]
    end

    subgraph "åè°ƒå™¨å±‚ (v0.5.0)"
        E[QueryProcessor<br/>æŸ¥è¯¢å¤„ç†åè°ƒå™¨]
        F[RetrievalCoordinator<br/>æ£€ç´¢åè°ƒå™¨]
        G[AnalysisCoordinator<br/>åˆ†æåè°ƒå™¨]
        H[VerdictGenerator<br/>è£å†³ç”Ÿæˆå™¨]
    end

    subgraph "æ£€ç´¢å±‚"
        I[EvidenceKnowledgeBase<br/>æœ¬åœ°å‘é‡çŸ¥è¯†åº“]
        J[HybridRetriever<br/>æ··åˆæ£€ç´¢å™¨]
        K[WebSearchTool<br/>è”ç½‘æœç´¢å·¥å…·]
    end

    subgraph "åˆ†æå±‚"
        L[QueryParser<br/>æŸ¥è¯¢æ„å›¾è§£æ]
        M[EvidenceAnalyzer<br/>è¯æ®è¯„ä¼°åˆ†æ]
        N[TruthSummarizer<br/>çœŸç›¸æ€»ç»“å™¨]
    end

    subgraph "åŸºç¡€è®¾æ–½å±‚"
        O[CacheManager<br/>åŒå±‚ç¼“å­˜ç®¡ç†]
        P[VersionManager<br/>ç‰ˆæœ¬ç®¡ç†]
        Q[LockManager<br/>é”ç®¡ç†å™¨]
        R[APIMonitor<br/>APIç›‘æ§]
        S[ParallelismConfig<br/>å¹¶è¡Œåº¦é…ç½®]
    end

    subgraph "çŸ¥è¯†ç®¡ç†å±‚"
        T[KnowledgeIntegrator<br/>çŸ¥è¯†é›†æˆå™¨]
    end

    A --> D
    B --> D
    C --> D
    D --> E
    D --> F
    D --> G
    D --> H
    E --> O
    E --> L
    F --> I
    F --> J
    F --> K
    G --> M
    H --> N
    J --> I
    J --> K
    D --> T
    D --> O
    D --> P
    D --> Q
    D --> R
    D --> S
```

### 1.3 ä¸»è¦å·¥ä½œæµç¨‹

```mermaid
sequenceDiagram
    participant User as ç”¨æˆ·
    participant Engine as RumorJudgeEngine
    participant QP as QueryProcessor
    participant RC as RetrievalCoordinator
    participant AC as AnalysisCoordinator
    participant VG as VerdictGenerator
    participant KB as çŸ¥è¯†åº“
    participant Cache as ç¼“å­˜ç®¡ç†å™¨

    User->>Engine: run(query)
    Engine->>Engine: _lazy_init() å»¶è¿Ÿåˆå§‹åŒ–

    rect rgb(200, 220, 240)
        Note over Engine,Cache: é˜¶æ®µ1: æŸ¥è¯¢å¤„ç†
        Engine->>QP: parse_with_parallel_retrieval(query)
        par å¹¶è¡Œæ‰§è¡Œ
            QP->>QP: LLMè§£ææ„å›¾
            QP->>KB: åŸå§‹è¯æŠ¢è·‘æ£€ç´¢
        end
        QP-->>Engine: (parsed, local_docs)
    end

    rect rgb(220, 240, 200)
        Note over Engine,Cache: é˜¶æ®µ2: ç¼“å­˜æ£€æŸ¥
        Engine->>Cache: get_verdict(query)
        alt ç¼“å­˜å‘½ä¸­
            Cache-->>Engine: cached verdict
            Engine-->>User: è¿”å›ç¼“å­˜ç»“æœ
        end
    end

    rect rgb(240, 220, 200)
        Note over Engine,KB: é˜¶æ®µ3: è¯æ®æ£€ç´¢
        Engine->>RC: retrieve_with_parsed_query()
        RC->>KB: è§£æè¯è¡¥æµ‹
        RC->>RC: æ··åˆæ£€ç´¢(æœ¬åœ°+è”ç½‘)
        RC-->>Engine: evidence_list
    end

    rect rgb(240, 200, 220)
        Note over Engine,VG: é˜¶æ®µ4: è¯æ®åˆ†æ
        Engine->>AC: analyze(claim, evidence_list)
        AC->>AC: å¹¶è¡Œåˆ†æè¯æ®
        AC-->>Engine: assessments
    end

    rect rgb(220, 200, 240)
        Note over Engine,Cache: é˜¶æ®µ5: è£å†³ç”Ÿæˆ
        Engine->>VG: generate()
        VG->>VG: ç»¼åˆè¯æ®ç”Ÿæˆè£å†³
        VG-->>Engine: FinalVerdict
        Engine->>Cache: set_verdict()
    end

    rect rgb(240, 240, 200)
        Note over Engine: é˜¶æ®µ6: çŸ¥è¯†é›†æˆ(å¼‚æ­¥)
        Engine->>Engine: _auto_integrate_knowledge()
        Note right of Engine: åå°çº¿ç¨‹æ‰§è¡Œ
    end

    Engine-->>User: UnifiedVerificationResult
```

**æµç¨‹è¯´æ˜ï¼š**

1. **æŸ¥è¯¢å¤„ç†é˜¶æ®µ**ï¼šå¹¶è¡Œæ‰§è¡ŒLLMæ„å›¾è§£æå’Œæœ¬åœ°å‘é‡æ£€ç´¢ï¼ˆæŠ¢è·‘ç­–ç•¥ï¼‰
2. **ç¼“å­˜æ£€æŸ¥é˜¶æ®µ**ï¼šç²¾ç¡®åŒ¹é… + è¯­ä¹‰ç›¸ä¼¼åº¦åŒ¹é…ï¼Œç‰ˆæœ¬æ„ŸçŸ¥å¤±æ•ˆ
3. **è¯æ®æ£€ç´¢é˜¶æ®µ**ï¼šæ··åˆæ£€ç´¢ç­–ç•¥ï¼Œæœ¬åœ°è´¨é‡ä¸è¶³æ—¶è‡ªåŠ¨è”ç½‘
4. **è¯æ®åˆ†æé˜¶æ®µ**ï¼šå¤šè§’åº¦å¹¶è¡Œåˆ†æï¼ŒåŠ¨æ€è°ƒæ•´å¹¶è¡Œåº¦
5. **è£å†³ç”Ÿæˆé˜¶æ®µ**ï¼šç»¼åˆæ‰€æœ‰è¯æ®ç»™å‡ºæœ€ç»ˆè£å†³
6. **çŸ¥è¯†é›†æˆé˜¶æ®µ**ï¼šåå°å¼‚æ­¥å°†é«˜ç½®ä¿¡åº¦ç»“æœè½¬åŒ–ä¸ºæœ¬åœ°çŸ¥è¯†

### 1.4 æŠ€æœ¯æ ˆ

| ç±»åˆ« | æŠ€æœ¯ç»„ä»¶ | ç”¨é€” |
|------|---------|------|
| **æ ¸å¿ƒæ¡†æ¶** | Python 3.11+ | ä¸»è¦ç¼–ç¨‹è¯­è¨€ |
| **LLMæ¡†æ¶** | LangChain 0.3+ | LLMåº”ç”¨å¼€å‘æ¡†æ¶ |
| **å‘é‡æ•°æ®åº“** | ChromaDB | æœ¬åœ°å‘é‡çŸ¥è¯†åº“å­˜å‚¨ |
| **åµŒå…¥æ¨¡å‹** | text-embedding-v4 (DashScope) | æ–‡æœ¬å‘é‡åŒ– |
| **LLMæ¨¡å‹** | qwen-max, qwen-plus (é€šä¹‰åƒé—®) | æ„å›¾è§£æã€è¯æ®åˆ†æã€è£å†³ç”Ÿæˆ |
| **è”ç½‘æœç´¢** | Tavily API | äº’è”ç½‘æœç´¢ |
| **ç¼“å­˜** | diskcache | ç£ç›˜æŒä¹…åŒ–ç¼“å­˜ |
| **å¹¶å‘** | concurrent.futures | å¹¶è¡Œå¤„ç† |
| **æ—¥å¿—** | structlog | ç»“æ„åŒ–æ—¥å¿— |

---

## ç¬¬äºŒéƒ¨åˆ†ï¼šæ ¸å¿ƒæ¨¡å—è¯¦è§£

### 2.1 å¼•æ“å±‚ - RumorJudgeEngine

**æ–‡ä»¶ä½ç½®ï¼š** `src/core/pipeline.py`

#### èƒŒæ™¯ä¸ç›®æ ‡

**ä¸ºä»€ä¹ˆéœ€è¦å¼•æ“å±‚ï¼Ÿ**
- è°£è¨€æ ¸æŸ¥æ˜¯ä¸€ä¸ªå¤æ‚çš„å¤šé˜¶æ®µæµç¨‹ï¼Œéœ€è¦ä¸€ä¸ªç»Ÿä¸€çš„ç¼–æ’è€…
- å„ä¸ªåŠŸèƒ½æ¨¡å—ï¼ˆè§£æã€æ£€ç´¢ã€åˆ†æã€è£å†³ï¼‰éœ€è¦åè°ƒå·¥ä½œ
- éœ€è¦ç»Ÿä¸€çš„ç”Ÿå‘½å‘¨æœŸç®¡ç†å’Œé”™è¯¯å¤„ç†æœºåˆ¶

**è§£å†³äº†ä»€ä¹ˆé—®é¢˜ï¼Ÿ**
- **å¤æ‚æµç¨‹ç¼–æ’**ï¼šå°†6ä¸ªé˜¶æ®µçš„å¤„ç†æµç¨‹ç»Ÿä¸€ç®¡ç†
- **ç»„ä»¶ç”Ÿå‘½å‘¨æœŸ**ï¼šå»¶è¿Ÿåˆå§‹åŒ–ï¼ŒæŒ‰éœ€åŠ è½½èµ„æº
- **ç»Ÿä¸€é”™è¯¯å¤„ç†**ï¼šæ ‡å‡†åŒ–é”™è¯¯å“åº”å’Œå…ƒæ•°æ®è®°å½•
- **çº¿ç¨‹å®‰å…¨**ï¼šå•ä¾‹æ¨¡å¼ + ç»†ç²’åº¦é”ï¼Œæ”¯æŒå¤šçº¿ç¨‹å¹¶å‘

#### æ¶æ„è®¾è®¡

```mermaid
classDiagram
    class RumorJudgeEngine {
        -_instance: RumorJudgeEngine
        -_singleton_lock: Lock
        -_lock_mgr: LockManager
        -_kb: EvidenceKnowledgeBase
        -_cache_manager: CacheManager
        -_hybrid_retriever: HybridRetriever
        -_query_processor: QueryProcessor
        -_retrieval_coordinator: RetrievalCoordinator
        -_analysis_coordinator: AnalysisCoordinator
        -_verdict_generator: VerdictGenerator
        +run(query, use_cache) UnifiedVerificationResult
        -_lazy_init() void
        -_run_with_coordinators() UnifiedVerificationResult
        -_auto_integrate_knowledge() void
    }

    class UnifiedVerificationResult {
        +query: str
        +entity: str
        +claim: str
        +final_verdict: str
        +confidence_score: int
        +retrieved_evidence: List
        +evidence_assessments: List
        +is_cached: bool
        +is_web_search: bool
        +add_metadata() void
    }

    class PipelineStage {
        <<enumeration>>
        CACHE_CHECK
        PARSING
        RETRIEVAL
        WEB_SEARCH
        ANALYSIS
        VERDICT
    }

    RumorJudgeEngine --> UnifiedVerificationResult : åˆ›å»º
    RumorJudgeEngine --> PipelineStage : ä½¿ç”¨
```

#### å…³é”®å®ç°

**1. çº¿ç¨‹å®‰å…¨çš„å•ä¾‹æ¨¡å¼**

```python
class RumorJudgeEngine:
    _instance = None
    _singleton_lock = threading.Lock()  # å•ä¾‹åˆ›å»ºä¸“ç”¨é”

    def __new__(cls):
        """å®ç°å•ä¾‹æ¨¡å¼ï¼Œç¡®ä¿å…¨å±€åªæœ‰ä¸€ä¸ªå¼•æ“å®ä¾‹"""
        with cls._singleton_lock:
            if cls._instance is None:
                cls._instance = super(RumorJudgeEngine, cls).__new__(cls)
                cls._instance._initialized = False
            return cls._instance
```

**è®¾è®¡ç†ç”±ï¼š**
- **ä¸ºä»€ä¹ˆç”¨å•ä¾‹ï¼Ÿ** å¼•æ“åˆå§‹åŒ–æˆæœ¬é«˜ï¼ˆåŠ è½½å‘é‡åº“ã€åˆå§‹åŒ–LLMï¼‰ï¼Œå…¨å±€å…±äº«ä¸€ä¸ªå®ä¾‹æœ€ç»æµ
- **ä¸ºä»€ä¹ˆç”¨ç‹¬ç«‹é”ï¼Ÿ** å•ä¾‹åˆ›å»ºé”ä¸ç»„ä»¶åˆå§‹åŒ–é”åˆ†ç¦»ï¼Œé¿å…æ­»é”
- **åŒé‡æ£€æŸ¥æ¨¡å¼ï¼š** `__new__` ä¸­æ£€æŸ¥ï¼Œ`__init__` ä¸­å†æ£€æŸ¥ï¼Œç¡®ä¿çº¿ç¨‹å®‰å…¨

**2. å»¶è¿Ÿåˆå§‹åŒ–ï¼ˆLazy Initializationï¼‰**

```python
def _lazy_init(self):
    """å»¶è¿Ÿåˆå§‹åŒ–æ ¸å¿ƒç»„ä»¶ï¼Œç¡®ä¿åœ¨çœŸæ­£éœ€è¦æ—¶æ‰åŠ è½½èµ„æº"""
    if self._components_initialized:
        return

    with self._lock_mgr.acquire("component_init", timeout=30):
        # åŒé‡æ£€æŸ¥
        if self._components_initialized:
            return

        # åˆå§‹åŒ–æ‰€æœ‰æ ¸å¿ƒç»„ä»¶
        self._kb = EvidenceKnowledgeBase()
        self._cache_manager = CacheManager(embeddings=self._kb.embeddings)
        # ...
        self._components_initialized = True
```

**è®¾è®¡ç†ç”±ï¼š**
- **ä¸ºä»€ä¹ˆå»¶è¿Ÿåˆå§‹åŒ–ï¼Ÿ** é¿å…å¯¼å…¥æ—¶ç«‹å³åŠ è½½é‡å‹èµ„æºï¼ˆå‘é‡åº“ã€LLMï¼‰ï¼Œæé«˜å¯åŠ¨é€Ÿåº¦
- **ä¸ºä»€ä¹ˆç”¨åŒé‡æ£€æŸ¥ï¼Ÿ** å¹¶å‘åœºæ™¯ä¸‹ï¼Œç¬¬ä¸€ä¸ªçº¿ç¨‹åˆå§‹åŒ–å®Œæˆåï¼Œåç»­çº¿ç¨‹æ— éœ€ç­‰å¾…
- **ä¸ºä»€ä¹ˆç”¨LockManagerï¼Ÿ** ç»Ÿä¸€ç®¡ç†æ‰€æœ‰é”ï¼Œæ”¯æŒè¶…æ—¶æœºåˆ¶ï¼Œé¿å…æ­»é”

**3. è‡ªåŠ¨çŸ¥è¯†æ²‰æ·€**

```python
def _auto_integrate_knowledge(self, result: UnifiedVerificationResult):
    """
    è‡ªåŠ¨çŸ¥è¯†æ²‰æ·€ï¼šå¦‚æœé€šè¿‡è”ç½‘æœç´¢è·å¾—äº†é«˜ç½®ä¿¡åº¦çš„ç»“è®ºï¼Œ
    å°†å…¶å¼‚æ­¥è½¬åŒ–ä¸ºæœ¬åœ°çŸ¥è¯†ã€‚
    """
    min_confidence = getattr(config, 'AUTO_INTEGRATE_MIN_CONFIDENCE', 90)
    min_evidence = getattr(config, 'AUTO_INTEGRATE_MIN_EVIDENCE', 3)

    # ä¸¥æ ¼å‡†å…¥é—¨æ§›
    if result.final_verdict not in ["çœŸ", "å‡"]:
        return
    if result.confidence_score < min_confidence:
        return
    if len(result.retrieved_evidence) < min_evidence:
        return

    # åå°å¼‚æ­¥é›†æˆ
    def background_integration():
        with self._lock_mgr.acquire("knowledge_integration", timeout=1.0):
            # ç”ŸæˆçŸ¥è¯†æ–‡ä»¶å¹¶å¢é‡æ›´æ–°å‘é‡åº“
            self.knowledge_integrator.rebuild_knowledge_base()

    thread = threading.Thread(target=background_integration)
    thread.daemon = True
    thread.start()
```

**è®¾è®¡ç†ç”±ï¼š**
- **ä¸ºä»€ä¹ˆä¸¥æ ¼é—¨æ§›ï¼Ÿ** é¿å…ä½è´¨é‡ä¿¡æ¯æ±¡æŸ“æœ¬åœ°çŸ¥è¯†åº“
- **ä¸ºä»€ä¹ˆå¼‚æ­¥æ‰§è¡Œï¼Ÿ** ä¸é˜»å¡ç”¨æˆ·æŸ¥è¯¢ï¼Œæå‡å“åº”é€Ÿåº¦
- **ä¸ºä»€ä¹ˆç”¨daemonçº¿ç¨‹ï¼Ÿ** ä¸»ç¨‹åºé€€å‡ºæ—¶è‡ªåŠ¨ç»“æŸï¼Œé¿å…åƒµå°¸è¿›ç¨‹

#### æŠ€æœ¯éš¾ç‚¹ä¸è§£å†³æ–¹æ¡ˆ

| éš¾ç‚¹ | è§£å†³æ–¹æ¡ˆ |
|------|---------|
| **æ­»é”é£é™©** | ä½¿ç”¨LockManagerç»Ÿä¸€ç®¡ç†é”ï¼Œæ¯ä¸ªé”æœ‰åç§°å’Œè¶…æ—¶ |
| **ç»„ä»¶åˆå§‹åŒ–é¡ºåº** | å»¶è¿Ÿåˆå§‹åŒ–ï¼ŒæŒ‰éœ€åŠ è½½ï¼Œé¿å…å¾ªç¯ä¾èµ– |
| **å¹¶å‘å®‰å…¨** | ç»†ç²’åº¦é” + ä¸Šä¸‹æ–‡ç®¡ç†å™¨ï¼Œè‡ªåŠ¨é‡Šæ”¾é” |
| **èµ„æºæ³„æ¼** | daemonçº¿ç¨‹ + ä¸Šä¸‹æ–‡ç®¡ç†å™¨ï¼Œç¡®ä¿èµ„æºé‡Šæ”¾ |

---

### 2.2 åè°ƒå™¨æ¨¡å¼

**æ–‡ä»¶ä½ç½®ï¼š** `src/core/coordinators/`

#### èƒŒæ™¯ä¸ç›®æ ‡

**ä¸ºä»€ä¹ˆå¼•å…¥åè°ƒå™¨æ¨¡å¼ï¼Ÿ**

åœ¨v0.5.0ä¹‹å‰ï¼Œ`RumorJudgeEngine`æ‰¿æ‹…äº†å¤ªå¤šèŒè´£ï¼š
- æŸ¥è¯¢è§£æé€»è¾‘è€¦åˆåœ¨å¼•æ“ä¸­
- æ£€ç´¢ç­–ç•¥åˆ¤æ–­é€»è¾‘æ•£è½å„å¤„
- è¯æ®åˆ†æå¹¶è¡Œé€»è¾‘å¤æ‚

**åè°ƒå™¨æ¨¡å¼è§£å†³çš„æ ¸å¿ƒé—®é¢˜ï¼š**

1. **å…³æ³¨ç‚¹åˆ†ç¦»**ï¼šæ¯ä¸ªåè°ƒå™¨ä¸“æ³¨äºä¸€ä¸ªé¢†åŸŸ
2. **ä»£ç å¤ç”¨**ï¼šåè°ƒå™¨å¯ç‹¬ç«‹æµ‹è¯•å’Œå¤ç”¨
3. **æ˜“äºæ‰©å±•**ï¼šæ–°å¢åŠŸèƒ½åªéœ€ä¿®æ”¹å¯¹åº”åè°ƒå™¨

#### åè°ƒå™¨æ¶æ„

```mermaid
graph TB
    subgraph "åè°ƒå™¨å±‚æ¬¡ç»“æ„"
        A[BaseCoordinator<br/>æŠ½è±¡åŸºç±»]
        B[QueryProcessor<br/>æŸ¥è¯¢å¤„ç†åè°ƒå™¨]
        C[RetrievalCoordinator<br/>æ£€ç´¢åè°ƒå™¨]
        D[AnalysisCoordinator<br/>åˆ†æåè°ƒå™¨]
        E[VerdictGenerator<br/>è£å†³ç”Ÿæˆå™¨]
    end

    A --> B
    A --> C
    A --> D
    A --> E

    subgraph "QueryProcessorèŒè´£"
        B1[æŸ¥è¯¢æ„å›¾è§£æ]
        B2[ç¼“å­˜æ£€æŸ¥]
        B3[å¹¶è¡Œæ£€ç´¢]
    end

    subgraph "RetrievalCoordinatorèŒè´£"
        C1[æ··åˆæ£€ç´¢ç­–ç•¥]
        C2[è¯æ®å»é‡]
        C3[æ ¼å¼è½¬æ¢]
    end

    subgraph "AnalysisCoordinatorèŒè´£"
        D1[å¹¶è¡Œåˆ†æè°ƒåº¦]
        D2[åŠ¨æ€å¹¶è¡Œåº¦è°ƒæ•´]
    end

    subgraph "VerdictGeneratorèŒè´£"
        E1[è¯æ®ç»¼åˆ]
        E2[è£å†³ç”Ÿæˆ]
        E3[ç½®ä¿¡åº¦è®¡ç®—]
    end

    B --> B1
    B --> B2
    B --> B3
    C --> C1
    C --> C2
    C --> C3
    D --> D1
    D --> D2
    E --> E1
    E --> E2
    E --> E3
```

#### 2.2.1 QueryProcessor - æŸ¥è¯¢å¤„ç†åè°ƒå™¨

**æ–‡ä»¶ï¼š** `src/core/coordinators/query_processor.py`

**èŒè´£ï¼š**
1. è§£æç”¨æˆ·æŸ¥è¯¢ï¼ˆå®ä½“ã€ä¸»å¼ ã€åˆ†ç±»ï¼‰
2. æ£€æŸ¥ç¼“å­˜
3. **å¹¶è¡Œæ‰§è¡Œè§£æå’Œæœ¬åœ°æ£€ç´¢**ï¼ˆv0.5.1æ–°å¢ï¼‰

**æ ¸å¿ƒæ–¹æ³•ï¼š`parse_with_parallel_retrieval`**

```python
def parse_with_parallel_retrieval(self, query: str) -> Tuple[Optional[QueryAnalysis], list]:
    """
    å¹¶è¡Œæ‰§è¡ŒæŸ¥è¯¢è§£æå’Œæœ¬åœ°æ£€ç´¢ï¼ˆæŠ¢è·‘ç­–ç•¥ï¼‰

    ä¸ºä»€ä¹ˆå¹¶è¡Œï¼Ÿ
    - LLMè§£æéœ€è¦å‡ ç™¾æ¯«ç§’
    - æœ¬åœ°æ£€ç´¢ä¹Ÿéœ€è¦å‡ ç™¾æ¯«ç§’
    - ä¸¤è€…ç‹¬ç«‹ï¼Œå¯ä»¥å¹¶è¡Œæ‰§è¡Œ
    - æ£€ç´¢ç»“æœå¯ä»¥å¤ç”¨ï¼Œé¿å…åç»­é‡å¤æ£€ç´¢
    """
    if PARALLELISM_CONFIG_AVAILABLE:
        max_workers = get_parallelism_config().get_adaptive_workers(
            task_count=2,
            task_type='retrieval',
            min_workers=2
        )
    else:
        max_workers = 2

    with concurrent.futures.ThreadPoolExecutor(max_workers=max_workers) as executor:
        # ä»»åŠ¡1: LLMè§£ææ„å›¾
        parse_future = executor.submit(self.parser_chain.invoke, {"query": query})

        # ä»»åŠ¡2: åŸå§‹è¯ç›´æ¥å»æœ¬åœ°åº“æŸ¥ä¸€æŠŠï¼ˆæŠ¢è·‘ï¼‰
        raw_search_future = executor.submit(self.hybrid_retriever.search_local, query)

        # ç­‰å¾…è§£æå®Œæˆ
        analysis = parse_future.result()
        local_docs = raw_search_future.result()

        return analysis, local_docs
```

**è®¾è®¡ç†ç”±ï¼š**

1. **ä¸ºä»€ä¹ˆæŠ¢è·‘ï¼Ÿ**
   - LLMè§£æå¯èƒ½æ”¹å˜æŸ¥è¯¢è¯ï¼ˆå¦‚"ç»´ç”Ÿç´ Cé˜²æ„Ÿå†’"â†’"entity:ç»´ç”Ÿç´ C, claim:é¢„é˜²æ„Ÿå†’"ï¼‰
   - ä½†åŸè¯çš„æ£€ç´¢ç»“æœå¯èƒ½ä»æœ‰ä»·å€¼
   - æå‰æ£€ç´¢å¯ä»¥èŠ‚çœåç»­æ—¶é—´

2. **ä¸ºä»€ä¹ˆç”¨åŠ¨æ€å¹¶è¡Œåº¦ï¼Ÿ**
   - 2ä¸ªä»»åŠ¡ä¸éœ€è¦å¤ªå¤šçº¿ç¨‹
   - ä½†ä»ä½¿ç”¨é…ç½®ç³»ç»Ÿä¿æŒä¸€è‡´æ€§
   - `min_workers=2` ç¡®ä¿è‡³å°‘2ä¸ªçº¿ç¨‹

#### 2.2.2 RetrievalCoordinator - æ£€ç´¢åè°ƒå™¨

**æ–‡ä»¶ï¼š** `src/core/coordinators/retrieval_coordinator.py`

**èŒè´£ï¼š**
1. åè°ƒæœ¬åœ°æ£€ç´¢å’Œç½‘ç»œæœç´¢
2. å®ç°æ··åˆæ£€ç´¢ç­–ç•¥
3. è¯æ®å»é‡å’Œæ’åº
4. æ ¼å¼è½¬æ¢ï¼ˆDocument â†’ Dictï¼‰

**æ ¸å¿ƒæ–¹æ³•ï¼š`retrieve_with_parsed_query`**

```python
def retrieve_with_parsed_query(
    self,
    query: str,
    parsed_info: Any,
    local_docs: List = None
) -> List[Dict[str, Any]]:
    """
    ä½¿ç”¨è§£æåçš„æŸ¥è¯¢è¿›è¡Œæ£€ç´¢ï¼ˆv0.5.1 å¢å¼ºï¼‰

    æµç¨‹ï¼š
    1. å¦‚æœè§£æè¯â‰ åŸå§‹è¯ï¼Œç”¨è§£æè¯è¡¥æµ‹æœ¬åœ°åº“
    2. æ±‡æ€»æ‰€æœ‰æœ¬åœ°ç»“æœå¹¶å»é‡
    3. è°ƒç”¨æ··åˆæ£€ç´¢ï¼ˆä¼ å…¥å·²æœ‰æœ¬åœ°æ–‡æ¡£ï¼‰
    4. æ··åˆæ£€ç´¢å†³å®šæ˜¯å¦è§¦å‘è”ç½‘æœç´¢
    """
    # æ„é€ è§£æè¯
    parsed_query = f"{parsed_info.entity} {parsed_info.claim}"

    # è§£æè¯è¡¥æµ‹æœ¬åœ°åº“
    if parsed_query and parsed_query != query:
        local_docs.extend(self.hybrid_retriever.search_local(parsed_query))

    # å»é‡
    unique_local_docs = self._deduplicate_docs(local_docs)

    # æ··åˆæ£€ç´¢ï¼ˆä¼šå†³å®šæ˜¯å¦è”ç½‘ï¼‰
    documents = self.hybrid_retriever.search_hybrid(
        search_query,
        existing_local_docs=unique_local_docs
    )

    return self._convert_to_dict_format(documents)
```

**è®¾è®¡ç†ç”±ï¼š**

1. **ä¸ºä»€ä¹ˆè§£æè¯è¡¥æµ‹ï¼Ÿ**
   - LLMè§£æåçš„æŸ¥è¯¢è¯å¯èƒ½æ›´å‡†ç¡®
   - ä¾‹ï¼šåŸè¯"å–éš”å¤œæ°´ä¼šè‡´ç™Œå—ï¼Ÿ"â†’è§£æè¯"éš”å¤œæ°´ è‡´ç™Œ"
   - è§£æè¯å¯èƒ½æ£€ç´¢åˆ°æ›´ç›¸å…³çš„ç»“æœ

2. **ä¸ºä»€ä¹ˆä¼ å…¥å·²æœ‰æœ¬åœ°æ–‡æ¡£ï¼Ÿ**
   - é¿å…æ··åˆæ£€ç´¢å™¨é‡å¤æ£€ç´¢æœ¬åœ°åº“
   - æ··åˆæ£€ç´¢å™¨å¯ä»¥åŸºäºå·²æœ‰ç»“æœåˆ¤æ–­æ˜¯å¦éœ€è¦è”ç½‘

#### 2.2.3 AnalysisCoordinator - åˆ†æåè°ƒå™¨

**æ–‡ä»¶ï¼š** `src/core/coordinators/analysis_coordinator.py`

**èŒè´£ï¼š**
1. è°ƒåº¦è¯æ®åˆ†æä»»åŠ¡
2. åŠ¨æ€è°ƒæ•´å¹¶è¡Œåº¦
3. æ±‡æ€»åˆ†æç»“æœ

**æ ¸å¿ƒç®€åŒ–å®ç°ï¼š**

```python
class AnalysisCoordinator(BaseCoordinator):
    """åˆ†æåè°ƒå™¨ - ç®€åŒ–ç‰ˆå§”æ‰˜ç»™EvidenceAnalyzer"""

    def analyze(self, claim: str, evidence_list: List[Dict]) -> List[Any]:
        """
        æ‰§è¡Œè¯æ®åˆ†æï¼ˆå§”æ‰˜ç»™EvidenceAnalyzerï¼‰

        ä¸ºä»€ä¹ˆéœ€è¦åè°ƒå™¨ï¼Ÿ
        - æœªæ¥å¯ä»¥åœ¨è¿™é‡Œæ·»åŠ æ›´å¤šé€»è¾‘
        - ä¾‹å¦‚ï¼šåŠ¨æ€å¹¶è¡Œåº¦è°ƒæ•´ã€ç»“æœç¼“å­˜ã€é”™è¯¯é‡è¯•ç­‰
        - ç›®å‰æ˜¯ç®€å•å§”æ‰˜ï¼Œä¿æŒæ¶æ„ä¸€è‡´æ€§
        """
        from src.analyzers.evidence_analyzer import EvidenceAnalyzer

        analyzer = EvidenceAnalyzer()
        return analyzer.analyze(claim, evidence_list)
```

**è®¾è®¡ç†ç”±ï¼š**

- **ä¸ºä»€ä¹ˆçœ‹èµ·æ¥è¿™ä¹ˆç®€å•ï¼Ÿ**
  - åè°ƒå™¨æ¨¡å¼é¢„ç•™äº†æ‰©å±•ç©ºé—´
  - æœªæ¥å¯ä»¥åœ¨è¿™é‡Œæ·»åŠ æ›´å¤šé€»è¾‘ï¼ˆå¦‚åŠ¨æ€å¹¶è¡Œåº¦ã€ç»“æœç¼“å­˜ï¼‰
  - ç›®å‰ç®€å•å§”æ‰˜ï¼Œä¿æŒæ¶æ„ä¸€è‡´æ€§

#### 2.2.4 VerdictGenerator - è£å†³ç”Ÿæˆå™¨

**æ–‡ä»¶ï¼š** `src/core/coordinators/verdict_generator.py`

**èŒè´£ï¼š**
1. ç»¼åˆæ‰€æœ‰è¯æ®å’Œåˆ†æç»“æœ
2. ç”Ÿæˆæœ€ç»ˆè£å†³
3. è®¡ç®—ç½®ä¿¡åº¦å’Œé£é™©ç­‰çº§

**æ ¸å¿ƒæ–¹æ³•ï¼š**

```python
def generate(
    self,
    query: str,
    entity: str,
    claim: str,
    evidence_list: List[Dict],
    assessments: List[Any]
) -> FinalVerdict:
    """
    ç”Ÿæˆæœ€ç»ˆè£å†³

    æµç¨‹ï¼š
    1. æ•´ç†è¯æ®å’Œåˆ†æç»“æœ
    2. è°ƒç”¨TruthSummarizerç”Ÿæˆè£å†³
    3. è¿”å›FinalVerdictå¯¹è±¡
    """
    from src.analyzers.truth_summarizer import summarize_truth

    verdict = summarize_truth(
        query=query,
        entity=entity,
        claim=claim,
        evidence_list=evidence_list,
        assessments=assessments
    )

    return verdict
```

**è®¾è®¡ç†ç”±ï¼š**

- **ä¸ºä»€ä¹ˆç‹¬ç«‹æˆåè°ƒå™¨ï¼Ÿ**
  - è£å†³ç”Ÿæˆæ˜¯ä¸€ä¸ªå¤æ‚çš„é€»è¾‘
  - æœªæ¥å¯ä»¥æ”¯æŒå¤šç§è£å†³ç­–ç•¥ï¼ˆæŠ•ç¥¨ã€åŠ æƒã€æœºå™¨å­¦ä¹ ï¼‰
  - ä¸å¼•æ“è§£è€¦ï¼Œä¾¿äºæµ‹è¯•

---

### 2.3 ç¼“å­˜ç®¡ç†ç³»ç»Ÿ

**æ–‡ä»¶ä½ç½®ï¼š** `src/core/cache_manager.py`

#### èƒŒæ™¯ä¸ç›®æ ‡

**ä¸ºä»€ä¹ˆéœ€è¦åŒå±‚ç¼“å­˜ï¼Ÿ**

1. **ç²¾ç¡®åŒ¹é…ç¼“å­˜**ï¼šåŸºäºMD5å“ˆå¸Œï¼Œæ¯«ç§’çº§å“åº”
2. **è¯­ä¹‰ç›¸ä¼¼åº¦ç¼“å­˜**ï¼šåŸºäºå‘é‡ç›¸ä¼¼åº¦ï¼Œæ•è·ç›¸ä¼¼æŸ¥è¯¢

**è§£å†³äº†ä»€ä¹ˆé—®é¢˜ï¼Ÿ**

- **æ€§èƒ½ä¼˜åŒ–**ï¼šé¿å…é‡å¤çš„LLMè°ƒç”¨å’Œå‘é‡æ£€ç´¢
- **æˆæœ¬é™ä½**ï¼šå‡å°‘APIè°ƒç”¨æ¬¡æ•°
- **ç‰ˆæœ¬æ„ŸçŸ¥**ï¼šçŸ¥è¯†åº“æ›´æ–°åè‡ªåŠ¨å¤±æ•ˆç¼“å­˜

#### æ¶æ„è®¾è®¡

```mermaid
graph TB
    subgraph "CacheManager"
        A[CacheManager]
        B[ç²¾ç¡®ç¼“å­˜<br/>diskcache]
        C[è¯­ä¹‰ç¼“å­˜<br/>ChromaDB]
        D[VersionManager<br/>ç‰ˆæœ¬ç®¡ç†å™¨]
    end

    subgraph "ç¼“å­˜æŸ¥è¯¢æµç¨‹"
        E[ç”¨æˆ·æŸ¥è¯¢]
        F{ç²¾ç¡®ç¼“å­˜<br/>å‘½ä¸­?}
        G{è¯­ä¹‰ç¼“å­˜<br/>å‘½ä¸­?}
        H[æ‰§è¡Œå®Œæ•´æµç¨‹]
        I[è¿”å›ç¼“å­˜ç»“æœ]
    end

    A --> B
    A --> C
    A --> D

    E --> F
    F -->|æ˜¯| I
    F -->|å¦| G
    G -->|æ˜¯| I
    G -->|å¦| H
```

#### æ ¸å¿ƒå®ç°

**1. åŒå±‚ç¼“å­˜æŸ¥è¯¢**

```python
def get_verdict(self, query: str) -> Optional[FinalVerdict]:
    """
    å°è¯•è·å–ç¼“å­˜çš„è£å†³ç»“æœï¼ˆæ”¯æŒç²¾ç¡®åŒ¹é…å’Œè¯­ä¹‰åŒ¹é…ï¼‰

    æ–°å¢ï¼šæ£€æŸ¥çŸ¥è¯†åº“ç‰ˆæœ¬ä¸€è‡´æ€§ï¼Œå¦‚æœç‰ˆæœ¬ä¸åŒ¹é…åˆ™ç¼“å­˜å¤±æ•ˆ
    """
    # æ£€æŸ¥çŸ¥è¯†åº“ç‰ˆæœ¬æ˜¯å¦å˜åŒ–
    if self._is_version_changed():
        logger.info("çŸ¥è¯†åº“ç‰ˆæœ¬å·²å˜åŒ–ï¼Œç¼“å­˜å·²å¤±æ•ˆ")
        return None

    # 1. é¦–å…ˆå°è¯•ç²¾ç¡®åŒ¹é…ï¼ˆæé€Ÿï¼‰
    key = self._generate_key(query)
    data = self.cache.get(key)
    if data:
        if self._is_cache_version_valid(data):
            logger.info(f"ç²¾ç¡®å‘½ä¸­ç¼“å­˜: '{query}'")
            return self._to_verdict(data)
        else:
            # åˆ é™¤è¿‡æœŸç¼“å­˜
            self.cache.delete(key)

    # 2. å°è¯•è¯­ä¹‰åŒ¹é…ï¼ˆå¦‚æœé…ç½®äº† embeddingsï¼‰
    if self.vector_cache:
        results = self.vector_cache.similarity_search_with_score(query, k=1)
        if results:
            doc, distance = results[0]
            similarity = 1.0 - distance
            if similarity >= self.semantic_threshold:
                cached_query = doc.page_content
                semantic_key = doc.metadata.get("cache_key")
                logger.info(f"è¯­ä¹‰å‘½ä¸­ç¼“å­˜: '{query}' -> '{cached_query}' (ç›¸ä¼¼åº¦: {similarity:.4f})")

                semantic_data = self.cache.get(semantic_key)
                if semantic_data and self._is_cache_version_valid(semantic_data):
                    # ä¸ºäº†åŠ é€Ÿä¸‹æ¬¡åŒ¹é…ï¼Œå°†å½“å‰æŸ¥è¯¢ä¹Ÿå­˜å…¥ç²¾ç¡®ç¼“å­˜
                    self.cache.set(key, semantic_data, expire=self.default_ttl)
                    return self._to_verdict(semantic_data)

    return None
```

**è®¾è®¡ç†ç”±ï¼š**

1. **ä¸ºä»€ä¹ˆå…ˆç²¾ç¡®åŒ¹é…ï¼Ÿ**
   - ç²¾ç¡®åŒ¹é…O(1)å¤æ‚åº¦ï¼Œæœ€å¿«
   - è¯­ä¹‰åŒ¹é…éœ€è¦å‘é‡æ£€ç´¢ï¼Œè¾ƒæ…¢
   - å¤§å¤šæ•°é‡å¤æŸ¥è¯¢æ˜¯ç²¾ç¡®é‡å¤

2. **ä¸ºä»€ä¹ˆè¯­ä¹‰å‘½ä¸­åæ›´æ–°ç²¾ç¡®ç¼“å­˜ï¼Ÿ**
   - ä¸‹æ¬¡ç›¸åŒæŸ¥è¯¢ç›´æ¥å‘½ä¸­ç²¾ç¡®ç¼“å­˜
   - è‡ªåŠ¨"å­¦ä¹ "ç”¨æˆ·æŸ¥è¯¢æ¨¡å¼

3. **ä¸ºä»€ä¹ˆéœ€è¦ç‰ˆæœ¬æ„ŸçŸ¥ï¼Ÿ**
   - çŸ¥è¯†åº“æ›´æ–°åï¼Œæ—§ç¼“å­˜å¯èƒ½è¿‡æ—¶
   - ç‰ˆæœ¬ä¸åŒ¹é…æ—¶è‡ªåŠ¨å¤±æ•ˆ

**2. ç‰ˆæœ¬æ„ŸçŸ¥ç¼“å­˜å¤±æ•ˆ**

```python
def _is_version_changed(self) -> bool:
    """
    æ£€æŸ¥çŸ¥è¯†åº“ç‰ˆæœ¬æ˜¯å¦å˜åŒ–ï¼ˆçº¿ç¨‹å®‰å…¨ï¼‰

    è¾¹ç•Œæƒ…å†µå¤„ç†ï¼š
    - é¦–æ¬¡æ„å»ºï¼šNone -> æœ‰ç‰ˆæœ¬ï¼Œè§†ä¸ºå˜åŒ–
    - ç‰ˆæœ¬æ›´æ–°ï¼šæ—§ç‰ˆæœ¬ -> æ–°ç‰ˆæœ¬ï¼Œè§†ä¸ºå˜åŒ–
    - æ— ç‰ˆæœ¬æ–‡ä»¶ï¼šè§†ä¸ºæ— å˜åŒ–ï¼ˆä½¿ç”¨ TTL æœºåˆ¶ï¼‰
    """
    with self._version_lock:
        current_version = self._version_manager.get_current_version()

        old_version_id = self._current_kb_version.version_id if self._current_kb_version else None
        new_version_id = current_version.version_id if current_version else None

        if old_version_id != new_version_id:
            # æ›´æ–°å½“å‰ç‰ˆæœ¬
            self._current_kb_version = current_version
            return True

        return False

def _is_cache_version_valid(self, cached_data: dict) -> bool:
    """
    æ£€æŸ¥ç¼“å­˜æ¡ç›®çš„ç‰ˆæœ¬æ˜¯å¦æœ‰æ•ˆ

    è¾¹ç•Œæƒ…å†µå¤„ç†ï¼š
    - ç¼“å­˜æ— ç‰ˆæœ¬å· + å½“å‰æ— ç‰ˆæœ¬ï¼šæœ‰æ•ˆï¼ˆé¦–æ¬¡æ„å»ºå‰ï¼‰
    - ç¼“å­˜æ— ç‰ˆæœ¬å· + å½“å‰æœ‰ç‰ˆæœ¬ï¼šæ— æ•ˆï¼ˆé¦–æ¬¡æ„å»ºåçš„æ—§ç¼“å­˜ï¼‰
    - ç¼“å­˜æœ‰ç‰ˆæœ¬å· + ç‰ˆæœ¬ä¸åŒ¹é…ï¼šæ— æ•ˆ
    - ç¼“å­˜æœ‰ç‰ˆæœ¬å· + ç‰ˆæœ¬åŒ¹é…ï¼šæœ‰æ•ˆ
    """
    current_version = self._version_manager.get_current_version()
    current_version_id = current_version.version_id if current_version else None

    if "kb_version" not in cached_data:
        if current_version_id:
            return False  # æ—§ç‰ˆæœ¬ç¼“å­˜
        return True  # é¦–æ¬¡æ„å»ºå‰

    cached_version = cached_data.get("kb_version")
    if cached_version != current_version_id:
        return False

    return True
```

**è®¾è®¡ç†ç”±ï¼š**

1. **ä¸ºä»€ä¹ˆåŒºåˆ†é¦–æ¬¡æ„å»ºå‰åï¼Ÿ**
   - é¦–æ¬¡æ„å»ºå‰æ²¡æœ‰ç‰ˆæœ¬å·ï¼Œæ—§ç¼“å­˜åº”è¯¥ä¿ç•™
   - é¦–æ¬¡æ„å»ºåæœ‰ç‰ˆæœ¬å·ï¼Œæ—§ç¼“å­˜åº”è¯¥å¤±æ•ˆ

2. **ä¸ºä»€ä¹ˆç”¨é”ä¿æŠ¤ï¼Ÿ**
   - ç‰ˆæœ¬æ£€æŸ¥å¯èƒ½å¹¶å‘æ‰§è¡Œ
   - é¿å…è„è¯»å’Œä¸ä¸€è‡´

#### æŠ€æœ¯éš¾ç‚¹

| éš¾ç‚¹ | è§£å†³æ–¹æ¡ˆ |
|------|---------|
| **ç‰ˆæœ¬åŒæ­¥** | ç‰ˆæœ¬ç®¡ç†å™¨ + ç¼“å­˜æ¡ç›®æ ‡è®° |
| **è¾¹ç•Œæƒ…å†µ** | åŒºåˆ†é¦–æ¬¡æ„å»ºå‰åï¼Œæ­£ç¡®å¤„ç†None |
| **å¹¶å‘å®‰å…¨** | ç»†ç²’åº¦é” + åŒé‡æ£€æŸ¥ |

---

### 2.4 æ··åˆæ£€ç´¢ç³»ç»Ÿ

**æ–‡ä»¶ä½ç½®ï¼š** `src/retrievers/hybrid_retriever.py`

#### èƒŒæ™¯ä¸ç›®æ ‡

**ä¸ºä»€ä¹ˆéœ€è¦æ··åˆæ£€ç´¢ï¼Ÿ**

1. **æœ¬åœ°æ£€ç´¢å¿«ä½†çŸ¥è¯†æœ‰é™**ï¼šå‘é‡åº“åªåŒ…å«é¢„å­˜çš„çŸ¥è¯†
2. **ç½‘ç»œæœç´¢æ…¢ä½†çŸ¥è¯†æ— é™**ï¼šäº’è”ç½‘æœ‰æœ€æ–°ä¿¡æ¯
3. **éœ€è¦æ™ºèƒ½å†³ç­–**ï¼šä½•æ—¶è”ç½‘ã€ä½•æ—¶æœ¬åœ°

**è§£å†³äº†ä»€ä¹ˆé—®é¢˜ï¼Ÿ**

- **çŸ¥è¯†è¦†ç›–ä¸è¶³**ï¼šæœ¬åœ°æ²¡æœ‰æ—¶è‡ªåŠ¨è”ç½‘
- **æ£€ç´¢è´¨é‡ä½**ï¼šæœ¬åœ°ç›¸ä¼¼åº¦ä½æ—¶è‡ªåŠ¨è”ç½‘
- **ç»“æœé‡å¤**ï¼šè‡ªåŠ¨å»é‡æœ¬åœ°å’Œç½‘ç»œç»“æœ

#### æ¶æ„è®¾è®¡

```mermaid
flowchart TD
    A[ç”¨æˆ·æŸ¥è¯¢] --> B[æœ¬åœ°å‘é‡æ£€ç´¢]
    B --> C{æœ‰ç»“æœ?}
    C -->|å¦| D[è§¦å‘è”ç½‘æœç´¢]
    C -->|æ˜¯| E{æœ€é«˜ç›¸ä¼¼åº¦<br/>â‰¥ é˜ˆå€¼?}
    E -->|å¦| D
    E -->|æ˜¯| F[è·³è¿‡è”ç½‘]

    D --> G[è·å–ç½‘ç»œç»“æœ]
    G --> H[åˆå¹¶æœ¬åœ°+ç½‘ç»œ]
    F --> H

    H --> I[å“ˆå¸Œå»é‡]
    I --> J[å†…å®¹ç›¸ä¼¼åº¦å»é‡]
    J --> K[æ’åº]
    K --> L[è¿”å›Top-N]

    style D fill:#ffcccc
    style F fill:#ccffcc
```

#### æ ¸å¿ƒå®ç°

**1. æ··åˆæ£€ç´¢å†³ç­–**

```python
def search_hybrid(self, query: str, existing_local_docs: List[Document] = None) -> List[Document]:
    """æ‰§è¡Œæ··åˆæ£€ç´¢é€»è¾‘"""

    # 1. å¦‚æœæ²¡æœ‰ä¼ å…¥ç°æˆçš„æœ¬åœ°ç»“æœï¼Œåˆ™æ‰§è¡Œæœ¬åœ°æ£€ç´¢
    if existing_local_docs is None:
        all_docs = self.search_local(query)
    else:
        all_docs = existing_local_docs

    # 2. è®¡ç®—æœ€å¤§ç›¸ä¼¼åº¦ï¼ˆå¯¹è‡ªåŠ¨ç”Ÿæˆå†…å®¹é™æƒï¼‰
    max_similarity = 0.0
    if all_docs:
        weighted_similarities = []
        auto_gen_weight = getattr(config, 'AUTO_GEN_WEIGHT', 0.9)  # é»˜è®¤0.9
        for d in all_docs:
            raw_sim = d.metadata['similarity']
            # å¦‚æœæ˜¯è‡ªåŠ¨ç”Ÿæˆçš„å†…å®¹ï¼Œåº”ç”¨åŠ æƒç³»æ•°
            if "AUTO_GEN_" in d.metadata['source']:
                raw_sim *= auto_gen_weight
            weighted_similarities.append(raw_sim)
        max_similarity = max(weighted_similarities)

    # 3. åˆ¤æ–­æ˜¯å¦éœ€è¦è”ç½‘æœç´¢
    should_search_web = (
        len(all_docs) == 0 or
        max_similarity < self.min_local_similarity
    )

    if should_search_web:
        logger.info(f"è§¦å‘è”ç½‘æœç´¢ (ç›¸ä¼¼åº¦: {max_similarity:.2f} < {self.min_local_similarity})")
        web_results = self.web_tool.search(query)
        # è½¬æ¢ä¸ºDocumentæ ¼å¼...
        all_docs.extend(web_docs)

    # 4. å»é‡ä¸æ’åº
    unique_docs = self._deduplicate_docs(all_docs)
    sorted_docs = sorted(unique_docs, key=sort_key, reverse=True)
    return sorted_docs[:self.max_results]
```

**è®¾è®¡ç†ç”±ï¼š**

1. **ä¸ºä»€ä¹ˆå¯¹è‡ªåŠ¨ç”Ÿæˆå†…å®¹é™æƒï¼Ÿ**
   - è‡ªåŠ¨ç”Ÿæˆå†…å®¹ï¼ˆAUTO_GEN_*ï¼‰æ˜¯ç³»ç»Ÿè‡ªæˆ‘è¿›åŒ–çš„äº§ç‰©
   - å¯èƒ½ä¸å¦‚äººå·¥æ ¸æŸ¥çš„å†…å®¹å¯é 
   - é™æƒé¿å…å…¶è™šé«˜é˜»æ–­è”ç½‘æœç´¢

2. **ä¸ºä»€ä¹ˆç›¸ä¼¼åº¦é˜ˆå€¼è®¾ä¸º0.6ï¼Ÿ**
   - å¤ªé«˜ï¼ˆå¦‚0.8ï¼‰ï¼šé¢‘ç¹è§¦å‘è”ç½‘ï¼Œæ€§èƒ½å·®
   - å¤ªä½ï¼ˆå¦‚0.3ï¼‰ï¼šæœ¬åœ°è´¨é‡ä¸è¶³æ—¶ä»ä¸è”ç½‘
   - 0.6æ˜¯ç»éªŒå€¼ï¼Œå¹³è¡¡æ€§èƒ½å’Œè´¨é‡

**2. æ™ºèƒ½å»é‡**

```python
def _deduplicate_docs(self, docs: List[Document]) -> List[Document]:
    """
    æ™ºèƒ½å»é‡ï¼šç»“åˆå“ˆå¸Œå’Œå†…å®¹ç›¸ä¼¼åº¦åˆ¤æ–­

    ç­–ç•¥ï¼š
    1. ä½¿ç”¨å®Œæ•´å†…å®¹çš„å“ˆå¸Œè¿›è¡Œç²¾ç¡®å»é‡
    2. å¯¹å‰©ä½™æ–‡æ¡£ä½¿ç”¨å†…å®¹ç›¸ä¼¼åº¦è¿›è¡Œæ¨¡ç³Šå»é‡ï¼ˆ> 0.85è§†ä¸ºé‡å¤ï¼‰
    """
    if not docs:
        return []

    # ç¬¬ä¸€é˜¶æ®µï¼šç²¾ç¡®å“ˆå¸Œå»é‡
    seen_hashes = set()
    hash_unique = []
    for doc in docs:
        content = doc.page_content[:500].strip()
        h = hash(content)
        if h not in seen_hashes:
            seen_hashes.add(h)
            hash_unique.append(doc)

    # ç¬¬äºŒé˜¶æ®µï¼šå†…å®¹ç›¸ä¼¼åº¦æ¨¡ç³Šå»é‡
    unique = []
    for doc in hash_unique:
        content_clean = ' '.join(doc.page_content.split())

        is_duplicate = False
        for seen_doc in unique:
            similarity = SequenceMatcher(
                None,
                content_clean[:300],
                ' '.join(seen_doc.page_content.split())[:300]
            ).ratio()

            if similarity > 0.85:
                logger.info(f"å‘ç°ç›¸ä¼¼æ–‡æ¡£ï¼Œå·²å»é‡ (ç›¸ä¼¼åº¦: {similarity:.2f})")
                is_duplicate = True
                break

        if not is_duplicate:
            unique.append(doc)

    return unique
```

**è®¾è®¡ç†ç”±ï¼š**

1. **ä¸ºä»€ä¹ˆä¸¤é˜¶æ®µå»é‡ï¼Ÿ**
   - å“ˆå¸Œå»é‡å¿«é€Ÿä½†åªèƒ½ç²¾ç¡®åŒ¹é…
   - ç›¸ä¼¼åº¦å»é‡æ…¢ä½†èƒ½æ•è·è¿‘ä¼¼é‡å¤
   - å…ˆå¿«åæ…¢ï¼Œæ€§èƒ½æœ€ä¼˜

2. **ä¸ºä»€ä¹ˆé˜ˆå€¼0.85ï¼Ÿ**
   - å¤ªé«˜ï¼ˆ0.95ï¼‰ï¼šæ¼æ‰é‡å¤
   - å¤ªä½ï¼ˆ0.7ï¼‰ï¼šè¯¯æ€ä¸åŒæ–‡æ¡£
   - 0.85æ˜¯ç»éªŒå€¼

#### æŠ€æœ¯éš¾ç‚¹

| éš¾ç‚¹ | è§£å†³æ–¹æ¡ˆ |
|------|---------|
| **è”ç½‘å†³ç­–** | ç›¸ä¼¼åº¦é˜ˆå€¼ + è‡ªåŠ¨ç”Ÿæˆå†…å®¹é™æƒ |
| **ç»“æœå»é‡ | ä¸¤é˜¶æ®µï¼šå“ˆå¸Œ + ç›¸ä¼¼åº¦ |
| **æ’åºç­–ç•¥ | æœ¬åœ°ä¼˜å…ˆ + ç›¸ä¼¼åº¦åŠ æƒ |

---

### 2.5 è¯æ®åˆ†æç³»ç»Ÿ

**æ–‡ä»¶ä½ç½®ï¼š** `src/analyzers/evidence_analyzer.py`

#### èƒŒæ™¯ä¸ç›®æ ‡

**ä¸ºä»€ä¹ˆéœ€è¦å¤šè§’åº¦åˆ†æï¼Ÿ**

è°£è¨€æ ¸æŸ¥ä¸èƒ½ç®€å•åœ°åˆ¤æ–­"çœŸ"æˆ–"å‡"ï¼Œéœ€è¦è€ƒè™‘ï¼š

1. **ç›¸å…³æ€§**ï¼šè¯æ®æ˜¯å¦çœŸçš„è®¨è®ºäº†è¿™ä¸ªä¸»å¼ ï¼Ÿ
2. **ç«‹åœº**ï¼šè¯æ®æ”¯æŒè¿˜æ˜¯åå¯¹ï¼Ÿ
3. **å¤æ‚æƒ…å†µ**ï¼šæ˜¯å¦å­˜åœ¨å¤¸å¤§ã€è¿‡æ—¶ã€æ–­ç« å–ä¹‰ï¼Ÿ
4. **æƒå¨æ€§**ï¼šæ¥æºæ˜¯å¦å¯é ï¼Ÿ
5. **ç½®ä¿¡åº¦**ï¼šåˆ†æç»“æœæœ‰å¤šå¤§æŠŠæ¡ï¼Ÿ

**è§£å†³äº†ä»€ä¹ˆé—®é¢˜ï¼Ÿ**

- **å•ä¸€ç»´åº¦ä¸è¶³**ï¼šä¸ä»…åˆ¤æ–­çœŸä¼ªï¼Œè¿˜è¯†åˆ«å¤æ‚æƒ…å†µ
- **å¹¶è¡Œåˆ†æ**ï¼šå¤šè¯æ®æ—¶å¹¶è¡Œå¤„ç†ï¼Œæå‡æ€§èƒ½
- **æˆæœ¬ä¼˜åŒ–**ï¼šé¢„è¿‡æ»¤æœºåˆ¶ï¼Œå‡å°‘LLMè°ƒç”¨

#### åˆ†æç»´åº¦

```mermaid
graph TB
    subgraph "è¯æ®åˆ†æç»´åº¦"
        A[ç›¸å…³æ€§ Relevance<br/>é«˜/ä¸­/ä½]
        B[ç«‹åœº Stance<br/>æ”¯æŒ/åå¯¹/ä¸­ç«‹]
        C[å¤æ‚æƒ…å†µ Complexity<br/>å¤¸å¤§/è¿‡æ—¶/æ–­ç« å–ä¹‰/ç­‰]
        D[æƒå¨æ€§ Authority<br/>1-5åˆ†]
        E[ç½®ä¿¡åº¦ Confidence<br/>0.0-1.0]
    end

    subgraph "å¤æ‚æƒ…å†µç±»å‹"
        C1[å¤¸å¤§å…¶è¯]
        C2[è¿‡æ—¶ä¿¡æ¯]
        C3[æ–­ç« å–ä¹‰]
        C4[æ•°æ®é”™è¯¯]
        C5[å·æ¢æ¦‚å¿µ]
        C6[éƒ¨åˆ†äº‹å®]
    end

    C --> C1
    C --> C2
    C --> C3
    C --> C4
    C --> C5
    C --> C6
```

#### æ ¸å¿ƒå®ç°

**1. å¹¶è¡Œåˆ†æç­–ç•¥**

```python
def analyze(self, claim: str, evidence_list: List[Dict], chunk_size: int = 2) -> List[EvidenceAssessment]:
    """
    æ‰§è¡Œåˆ†æï¼Œå¦‚æœè¯æ®è¾ƒå¤šåˆ™é‡‡ç”¨å¹¶è¡Œå¤„ç†ä»¥æé«˜é€Ÿåº¦ã€‚

    ç­–ç•¥ï¼š
    - 2-5ä¸ªè¯æ®ï¼šå•è¯æ®å¹¶è¡Œåˆ†æ
    - >5ä¸ªè¯æ®ï¼šåˆ†ç‰‡å¹¶è¡Œåˆ†æ
    - é¢„è¿‡æ»¤ï¼šé™ä½APIæˆæœ¬
    """
    if not evidence_list:
        return []

    # é¢„è¿‡æ»¤è¯æ®ï¼ˆé™ä½LLMè°ƒç”¨æ¬¡æ•°ï¼‰
    evidence_list = self._prefilter_evidence(claim, evidence_list)

    count = len(evidence_list)

    # ä¼˜å…ˆä½¿ç”¨å•è¯æ®å¹¶è¡Œåˆ†æï¼ˆé€‚ç”¨äº 2-5 ä¸ªè¯æ®ï¼‰
    if 2 <= count <= 5:
        logger.info(f"è¯æ®æ•°é‡({count})é€‚åˆå•è¯æ®å¹¶è¡Œåˆ†æ")
        return self._analyze_parallel_single(claim, evidence_list)

    if count <= chunk_size:
        # æ•°é‡è¾ƒå°‘ï¼Œç›´æ¥å•æ¬¡è¯·æ±‚
        return self._analyze_batch(claim, evidence_list, offset=0)

    # æ•°é‡è¾ƒå¤šï¼Œåˆ†ç‰‡å¹¶è¡Œåˆ†æ
    chunks = [evidence_list[i:i + chunk_size] for i in range(0, count, chunk_size)]

    # ä½¿ç”¨åŠ¨æ€å¹¶è¡Œåº¦é…ç½®
    if PARALLELISM_CONFIG_AVAILABLE:
        max_workers = get_parallelism_config().get_adaptive_workers(
            task_count=len(chunks),
            task_type='evidence_analyzer',
            min_workers=1
        )
    else:
        max_workers = min(len(chunks), 5)

    all_assessments = []
    with concurrent.futures.ThreadPoolExecutor(max_workers=max_workers) as executor:
        future_to_chunk = {
            executor.submit(self._analyze_batch, claim, chunk, i * chunk_size): i
            for i, chunk in enumerate(chunks)
        }

        for future in concurrent.futures.as_completed(future_to_chunk):
            try:
                batch_results = future.result()
                all_assessments.extend(batch_results)
            except Exception as e:
                logger.error(f"åˆ†ç‰‡åˆ†æå¤±è´¥: {e}")

    all_assessments.sort(key=lambda x: x.id)
    return all_assessments
```

**è®¾è®¡ç†ç”±ï¼š**

1. **ä¸ºä»€ä¹ˆ2-5ä¸ªè¯æ®ç”¨å•è¯æ®å¹¶è¡Œï¼Ÿ**
   - æ¯ä¸ªè¯æ®ç‹¬ç«‹åˆ†æï¼Œç²’åº¦æ›´ç»†
   - é¿å…ä¸€ä¸ªå¤±è´¥å½±å“å…¨éƒ¨
   - æ¯”åˆ†ç‰‡å¹¶è¡Œæ›´å¿«ï¼ˆå‡å°‘ä¸²è¡Œç­‰å¾…ï¼‰

2. **ä¸ºä»€ä¹ˆ>5ä¸ªè¯æ®ç”¨åˆ†ç‰‡å¹¶è¡Œï¼Ÿ**
   - å•è¯æ®å¹¶è¡Œä¼šåˆ›å»ºå¤ªå¤šçº¿ç¨‹
   - åˆ†ç‰‡å¹¶è¡Œæ›´ç»æµ
   - chunk_size=2æ˜¯å¹³è¡¡ç‚¹

**2. é¢„è¿‡æ»¤æœºåˆ¶**

```python
def _prefilter_evidence(self, claim: str, evidence_list: List[Dict]) -> List[Dict]:
    """
    é¢„è¿‡æ»¤è¯æ®ï¼ŒåŸºäºç®€å•è§„åˆ™å¿«é€Ÿç­›é€‰

    è§„åˆ™ï¼š
    1. è¿‡æ»¤ç›¸ä¼¼åº¦è¿‡ä½çš„è¯æ®
    2. é™åˆ¶æœ€å¤§è¯æ®æ•°é‡
    3. ä¼˜å…ˆä¿ç•™æœ¬åœ°è¯æ®ï¼ˆæƒå¨æ€§æ›´é«˜ï¼‰
    """
    if not self.enable_prefilter:
        return evidence_list

    # 1. è¿‡æ»¤ç›¸ä¼¼åº¦è¿‡ä½çš„è¯æ®
    filtered = []
    for ev in evidence_list:
        similarity = ev.get('metadata', {}).get('similarity', 0.0)
        if similarity >= self.prefilter_min_similarity or similarity == 0.0:
            filtered.append(ev)

    # 2. ä¼˜å…ˆä¿ç•™æœ¬åœ°è¯æ®ï¼ˆæƒå¨æ€§æ›´é«˜ï¼‰
    local_evidence = [ev for ev in filtered if ev.get('metadata', {}).get('type') == 'local']
    web_evidence = [ev for ev in filtered if ev.get('metadata', {}).get('type') == 'web']

    # 3. é™åˆ¶æœ€å¤§æ•°é‡ï¼ˆæœ¬åœ°è¯æ®ä¼˜å…ˆï¼‰
    selected = local_evidence[:self.prefilter_max_evidence]
    remaining_slots = self.prefilter_max_evidence - len(selected)

    if remaining_slots > 0 and web_evidence:
        selected.extend(web_evidence[:remaining_slots])

    logger.info(f"é¢„è¿‡æ»¤å®Œæˆ: {len(evidence_list)} -> {len(selected)} æ¡è¯æ®")
    return selected
```

**è®¾è®¡ç†ç”±ï¼š**

1. **ä¸ºä»€ä¹ˆè¿‡æ»¤ä½ç›¸ä¼¼åº¦è¯æ®ï¼Ÿ**
   - ç›¸ä¼¼åº¦<0.3çš„è¯æ®åŸºæœ¬æ— å…³
   - åˆ†æå®ƒä»¬æµªè´¹APIè°ƒç”¨
   - ç”¨æˆ·ä¹Ÿè®¤ä¸ºä¸ç›¸å…³

2. **ä¸ºä»€ä¹ˆé™åˆ¶5æ¡ï¼Ÿ**
   - è£å†³ç”Ÿæˆä¸éœ€è¦å¤ªå¤šè¯æ®
   - 3-5æ¡é«˜è´¨é‡è¯æ®è¶³å¤Ÿ
   - å‡å°‘APIè°ƒç”¨ï¼Œé™ä½æˆæœ¬

**3. Few-Shotæç¤ºå·¥ç¨‹**

```python
self.prompt = ChatPromptTemplate.from_messages([
    ("system", """ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„è°£è¨€æ ¸æŸ¥åˆ†æå¸ˆã€‚

    **åˆ†æç¤ºä¾‹ (Few-Shot)**:

    *   **ä¸»å¼ **: "åƒé¦™è•‰ä¼šå¾—ç™Œç—‡ï¼Œå› ä¸ºé¦™è•‰æœ‰è¾å°„ã€‚"
        **è¯æ®**: "é¦™è•‰ç¡®å®å«æœ‰å¾®é‡æ”¾å°„æ€§é’¾-40ï¼Œä½†å…¶å‰‚é‡æä½ï¼Œå¯¹äººä½“å®Œå…¨æ— å®³ã€‚" (æ¥æº: ç§‘æ™®ä¸­å›½)
        **åˆ†æ**: ç«‹åœº=åå¯¹, æ ‡ç­¾=å¤¸å¤§å…¶è¯/å·æ¢æ¦‚å¿µ, ç†ç”±=è¯æ®æ‰¿è®¤æœ‰è¾å°„äº‹å®ï¼Œä½†æŒ‡å‡ºäº†å‰‚é‡çš„å®‰å…¨æ€§ï¼Œä¸»å¼ æŠ›å¼€å‰‚é‡è°ˆæ¯’æ€§æ˜¯å¤¸å¤§å±å®³ã€‚, å¼•ç”¨="å…¶å‰‚é‡æä½ï¼Œå¯¹äººä½“å®Œå…¨æ— å®³", ç½®ä¿¡åº¦=0.95

    *   **ä¸»å¼ **: "æŸåœ°åˆšåˆšå‘ç”Ÿç‰¹å¤§çˆ†ç‚¸ï¼Œä¼¤äº¡æƒ¨é‡ï¼"
        **è¯æ®**: "ç½‘ä¼ è§†é¢‘å®ä¸º2015å¹´å¤©æ´¥æ¸¯çˆ†ç‚¸è§†é¢‘ï¼Œè¿‘æœŸè¯¥åœ°æ— ç›¸å…³è­¦æƒ…ã€‚" (æ¥æº: ç½‘è­¦è¾Ÿè°£)
        **åˆ†æ**: ç«‹åœº=åå¯¹, æ ‡ç­¾=è¿‡æ—¶ä¿¡æ¯, ç†ç”±=ä¸»å¼ ä½¿ç”¨äº†æ—§è§†é¢‘æ¥ä¼ªé€ æ–°äº‹ä»¶ï¼Œå±äºæ—§é—»æ–°ç‚’ã€‚, å¼•ç”¨="ç½‘ä¼ è§†é¢‘å®ä¸º2015å¹´å¤©æ´¥æ¸¯çˆ†ç‚¸è§†é¢‘", ç½®ä¿¡åº¦=0.98
    """),
    ("human", "**è°£è¨€ä¸»å¼ **ï¼š{claim}\n\n**å¾…åˆ†æè¯æ®åˆ—è¡¨**ï¼š{evidence_text}")
])
```

**è®¾è®¡ç†ç”±ï¼š**

- **ä¸ºä»€ä¹ˆç”¨Few-Shotï¼Ÿ**
  - æä¾›å…·ä½“ç¤ºä¾‹ï¼ŒLLMç†è§£æ›´å‡†ç¡®
  - ç¤ºä¾‹è¦†ç›–å…¸å‹åœºæ™¯ï¼ˆå¤¸å¤§ã€è¿‡æ—¶ï¼‰
  - æé«˜è¾“å‡ºä¸€è‡´æ€§

#### æŠ€æœ¯éš¾ç‚¹

| éš¾ç‚¹ | è§£å†³æ–¹æ¡ˆ |
|------|---------|
| **å¹¶è¡Œç­–ç•¥** | 2-5å•è¯æ®å¹¶è¡Œï¼Œ>5åˆ†ç‰‡å¹¶è¡Œ |
| **APIæˆæœ¬** | é¢„è¿‡æ»¤ + é™åˆ¶æ•°é‡ |
| **è¾“å‡ºä¸€è‡´æ€§** | Few-Shotæç¤ºå·¥ç¨‹ |

---

## ç¬¬ä¸‰éƒ¨åˆ†ï¼šåŸºç¡€è®¾æ–½

### 3.1 çº¿ç¨‹å®‰å…¨æœºåˆ¶

**æ–‡ä»¶ä½ç½®ï¼š** `src/core/thread_utils.py`

#### èƒŒæ™¯ä¸ç›®æ ‡

**ä¸ºä»€ä¹ˆéœ€è¦çº¿ç¨‹å®‰å…¨ï¼Ÿ**

1. **å•ä¾‹æ¨¡å¼**ï¼šå¤šçº¿ç¨‹åŒæ—¶åˆ›å»ºå•ä¾‹
2. **å»¶è¿Ÿåˆå§‹åŒ–**ï¼šå¤šçº¿ç¨‹åŒæ—¶åˆå§‹åŒ–ç»„ä»¶
3. **çŸ¥è¯†é›†æˆ**ï¼šå¤šçº¿ç¨‹åŒæ—¶æ›´æ–°å‘é‡åº“
4. **ç‰ˆæœ¬æ£€æŸ¥**ï¼šå¤šçº¿ç¨‹åŒæ—¶æ£€æŸ¥ç‰ˆæœ¬

**è§£å†³äº†ä»€ä¹ˆé—®é¢˜ï¼Ÿ**

- **ç«æ€æ¡ä»¶**ï¼šå¤šçº¿ç¨‹åŒæ—¶ä¿®æ”¹å…±äº«çŠ¶æ€
- **æ­»é”**ï¼šå¤šä¸ªé”ç›¸äº’ç­‰å¾…
- **èµ„æºæ³„æ¼**ï¼šé”æœªæ­£ç¡®é‡Šæ”¾

#### LockManagerè®¾è®¡

```mermaid
graph TB
    subgraph "LockManager"
        A[LockManager]
        B[å‘½åé”å­—å…¸<br/>_locks: Dict[str, Lock]]
        C[è·å–é”<br/>acquire]
        D[é‡Šæ”¾é”<br/>release]
    end

    subgraph "é”å‘½åç©ºé—´"
        E["component_init<br/>ç»„ä»¶åˆå§‹åŒ–"]
        F["knowledge_integration<br/>çŸ¥è¯†é›†æˆ"]
        G["version_check<br/>ç‰ˆæœ¬æ£€æŸ¥"]
    end

    A --> B
    A --> C
    A --> D

    C --> E
    C --> F
    C --> G
```

#### æ ¸å¿ƒå®ç°

```python
class LockManager:
    """
    çº¿ç¨‹é”ç®¡ç†å™¨

    åŠŸèƒ½ï¼š
    1. ä¸ºä¸åŒçš„èµ„æºåˆ›å»ºç‹¬ç«‹çš„å‘½åé”
    2. æ”¯æŒè¶…æ—¶æœºåˆ¶ï¼Œé¿å…æ­»é”
    3. è‡ªåŠ¨é‡Šæ”¾é”ï¼ˆä¸Šä¸‹æ–‡ç®¡ç†å™¨ï¼‰
    4. é”å¤ç”¨ï¼Œé¿å…åˆ›å»ºè¿‡å¤šé”
    """

    def __init__(self):
        self._locks: Dict[str, threading.Lock] = {}
        self._meta_lock = threading.Lock()  # ä¿æŠ¤é”å­—å…¸æœ¬èº«çš„é”

    def acquire(self, name: str, timeout: float = None):
        """
        è·å–æŒ‡å®šåç§°çš„é”ï¼ˆä¸Šä¸‹æ–‡ç®¡ç†å™¨ï¼‰

        Args:
            name: é”åç§°
            timeout: è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰

        Returns:
            ä¸Šä¸‹æ–‡ç®¡ç†å™¨ï¼Œæ”¯æŒ with è¯­å¥
        """
        # è·å–æˆ–åˆ›å»ºé”
        with self._meta_lock:
            if name not in self._locks:
                self._locks[name] = threading.RLock()
            lock = self._locks[name]

        return _LockContext(lock, name, timeout)

class _LockContext:
    """é”ä¸Šä¸‹æ–‡ç®¡ç†å™¨"""

    def __init__(self, lock: threading.RLock, name: str, timeout: float = None):
        self.lock = lock
        self.name = name
        self.timeout = timeout
        self.acquired = False

    def __enter__(self):
        if self.timeout:
            self.acquired = self.lock.acquire(blocking=True, timeout=self.timeout)
            if not self.acquired:
                raise TimeoutError(f"è·å–é” '{self.name}' è¶…æ—¶")
        else:
            self.lock.acquire()
            self.acquired = True
        return self

    def __exit__(self, exc_type, exc_val, exc_tb):
        if self.acquired:
            self.lock.release()
```

**è®¾è®¡ç†ç”±ï¼š**

1. **ä¸ºä»€ä¹ˆç”¨å‘½åé”ï¼Ÿ**
   - ä¸åŒèµ„æºç”¨ä¸åŒé”ï¼Œå‡å°‘é”ç«äº‰
   - ä¾‹å¦‚ï¼š`component_init` å’Œ `knowledge_integration` ç‹¬ç«‹
   - é¿å…ä¸å¿…è¦çš„ç­‰å¾…

2. **ä¸ºä»€ä¹ˆç”¨RLockï¼Ÿ**
   - RLockæ˜¯å¯é‡å…¥é”
   - åŒä¸€çº¿ç¨‹å¯ä»¥å¤šæ¬¡è·å–
   - é¿å…æ­»é”

3. **ä¸ºä»€ä¹ˆéœ€è¦ä¸Šä¸‹æ–‡ç®¡ç†å™¨ï¼Ÿ**
   - è‡ªåŠ¨é‡Šæ”¾é”ï¼Œå³ä½¿å‘ç”Ÿå¼‚å¸¸
   - é¿å…æ‰‹åŠ¨releaseçš„é—æ¼

#### ä½¿ç”¨ç¤ºä¾‹

```python
# è·å–é”å¹¶ä½¿ç”¨ï¼ˆè‡ªåŠ¨é‡Šæ”¾ï¼‰
with self._lock_mgr.acquire("component_init", timeout=30) as lock:
    # æ‰§è¡Œéœ€è¦ä¿æŠ¤çš„æ“ä½œ
    self._kb = EvidenceKnowledgeBase()
    # é”ä¼šåœ¨withå—ç»“æŸæ—¶è‡ªåŠ¨é‡Šæ”¾
```

#### æŠ€æœ¯éš¾ç‚¹

| éš¾ç‚¹ | è§£å†³æ–¹æ¡ˆ |
|------|---------|
| **æ­»é”** | å‘½åé” + RLock + è¶…æ—¶ |
| **é”æ³„æ¼** | ä¸Šä¸‹æ–‡ç®¡ç†å™¨è‡ªåŠ¨é‡Šæ”¾ |
| **é”ç«äº‰** | ç»†ç²’åº¦é”ï¼Œå‡å°‘å†²çª |

---

### 3.2 ç‰ˆæœ¬ç®¡ç†ç³»ç»Ÿ

**æ–‡ä»¶ä½ç½®ï¼š** `src/core/version_manager.py`

#### èƒŒæ™¯ä¸ç›®æ ‡

**ä¸ºä»€ä¹ˆéœ€è¦ç‰ˆæœ¬ç®¡ç†ï¼Ÿ**

1. **ç¼“å­˜å¤±æ•ˆ**ï¼šçŸ¥è¯†åº“æ›´æ–°åï¼Œæ—§ç¼“å­˜åº”è¯¥å¤±æ•ˆ
2. **å¢é‡æ›´æ–°**ï¼šæ”¯æŒçŸ¥è¯†åº“çš„å¢é‡æ„å»º
3. **å›æ»šæ”¯æŒ**ï¼šç‰ˆæœ¬è®°å½•ä¾¿äºé—®é¢˜æ’æŸ¥

**è§£å†³äº†ä»€ä¹ˆé—®é¢˜ï¼Ÿ**

- **ç¼“å­˜è¿‡æœŸ**ï¼šç‰ˆæœ¬ä¸åŒ¹é…æ—¶è‡ªåŠ¨å¤±æ•ˆ
- **ä¸€è‡´æ€§**ï¼šç¡®ä¿ç¼“å­˜ä¸çŸ¥è¯†åº“ç‰ˆæœ¬ä¸€è‡´
- **å¯è¿½æº¯**ï¼šè®°å½•æ¯æ¬¡æ›´æ–°çš„æ—¶é—´å’Œæ¥æº

#### ç‰ˆæœ¬ä¿¡æ¯ç»“æ„

```mermaid
classDiagram
    class KnowledgeVersion {
        +version_id: str
        +created_at: datetime
        +doc_count: int
        +source: str
        +metadata: Dict
    }

    class VersionManager {
        +get_current_version() KnowledgeVersion
        +create_new_version() KnowledgeVersion
        +increment_version() KnowledgeVersion
    }

    VersionManager --> KnowledgeVersion : ç®¡ç†
```

#### æ ¸å¿ƒå®ç°

```python
class KnowledgeVersion(BaseModel):
    """çŸ¥è¯†åº“ç‰ˆæœ¬ä¿¡æ¯"""
    version_id: str = Field(description="ç‰ˆæœ¬å·ï¼Œæ ¼å¼ä¸º YYYYMMDD_HHMMSS")
    created_at: datetime = Field(description="åˆ›å»ºæ—¶é—´")
    doc_count: int = Field(description="æ–‡æ¡£æ•°é‡")
    source: str = Field(description="ç‰ˆæœ¬æ¥æºï¼Œå¦‚ 'auto_integrate' æˆ– 'manual_build'")
    metadata: Dict[str, Any] = Field(default_factory=dict, description="é¢å¤–å…ƒæ•°æ®")

class VersionManager:
    """
    çŸ¥è¯†åº“ç‰ˆæœ¬ç®¡ç†å™¨

    åŠŸèƒ½ï¼š
    1. è®°å½•çŸ¥è¯†åº“çš„ç‰ˆæœ¬ä¿¡æ¯
    2. æ”¯æŒå¢é‡æ›´æ–°
    3. æä¾›ç‰ˆæœ¬æŸ¥è¯¢
    """

    VERSION_FILE = "kb_version.json"

    def get_current_version(self) -> Optional[KnowledgeVersion]:
        """è·å–å½“å‰çŸ¥è¯†åº“ç‰ˆæœ¬"""
        version_file = self.base_dir / self.VERSION_FILE
        if not version_file.exists():
            return None

        try:
            with open(version_file, 'r', encoding='utf-8') as f:
                data = json.load(f)
            return KnowledgeVersion(**data)
        except Exception as e:
            logger.error(f"è¯»å–ç‰ˆæœ¬æ–‡ä»¶å¤±è´¥: {e}")
            return None

    def create_new_version(
        self,
        doc_count: int,
        source: str = "manual",
        metadata: Dict = None
    ) -> KnowledgeVersion:
        """åˆ›å»ºæ–°ç‰ˆæœ¬"""
        now = datetime.now()
        version_id = now.strftime("%Y%m%d_%H%M%S")

        version = KnowledgeVersion(
            version_id=version_id,
            created_at=now,
            doc_count=doc_count,
            source=source,
            metadata=metadata or {}
        )

        # ä¿å­˜ç‰ˆæœ¬æ–‡ä»¶
        version_file = self.base_dir / self.VERSION_FILE
        with open(version_file, 'w', encoding='utf-8') as f:
            json.dump(version.model_dump(), f, ensure_ascii=False, indent=2)

        logger.info(f"åˆ›å»ºæ–°ç‰ˆæœ¬: {version_id} (æ–‡æ¡£æ•°: {doc_count}, æ¥æº: {source})")
        return version
```

**è®¾è®¡ç†ç”±ï¼š**

1. **ä¸ºä»€ä¹ˆç”¨æ—¶é—´æˆ³ä½œä¸ºç‰ˆæœ¬å·ï¼Ÿ**
   - ç®€å•æ˜“è¯»
   - è‡ªåŠ¨é€’å¢
   - æ— éœ€é¢å¤–çŠ¶æ€ç®¡ç†

2. **ä¸ºä»€ä¹ˆè®°å½•æ¥æºï¼Ÿ**
   - åŒºåˆ†è‡ªåŠ¨é›†æˆå’Œæ‰‹åŠ¨æ„å»º
   - ä¾¿äºé—®é¢˜æ’æŸ¥

#### ä¸ç¼“å­˜é›†æˆ

```python
class CacheManager:
    def _is_version_changed(self) -> bool:
        """æ£€æŸ¥çŸ¥è¯†åº“ç‰ˆæœ¬æ˜¯å¦å˜åŒ–"""
        current_version = self._version_manager.get_current_version()
        old_version_id = self._current_kb_version.version_id if self._current_kb_version else None
        new_version_id = current_version.version_id if current_version else None

        if old_version_id != new_version_id:
            self._current_kb_version = current_version
            return True
        return False
```

#### æŠ€æœ¯éš¾ç‚¹

| éš¾ç‚¹ | è§£å†³æ–¹æ¡ˆ |
|------|---------|
| **ç‰ˆæœ¬åŒæ­¥ | æ¯æ¬¡æ£€æŸ¥å½“å‰ç‰ˆæœ¬ï¼Œæ¯”è¾ƒç‰ˆæœ¬ID |
| **é¦–æ¬¡æ„å»º | åŒºåˆ†Noneå’Œæœ‰ç‰ˆæœ¬ï¼Œæ­£ç¡®å¤„ç†è¾¹ç•Œ |
| **å¹¶å‘è®¿é—® | æ–‡ä»¶è¯»å†™ç”¨é”ä¿æŠ¤ |

---

### 3.3 APIç›‘æ§ç³»ç»Ÿ

**æ–‡ä»¶ä½ç½®ï¼š** `src/observability/api_monitor.py`

#### èƒŒæ™¯ä¸ç›®æ ‡

**ä¸ºä»€ä¹ˆéœ€è¦APIç›‘æ§ï¼Ÿ**

1. **æˆæœ¬æ§åˆ¶**ï¼šè¿½è¸ªAPIè°ƒç”¨çš„è´¹ç”¨
2. **é…é¢ç®¡ç†**ï¼šé¿å…è¶…è¿‡æ¯æ—¥é™é¢
3. **æ€§èƒ½åˆ†æ**ï¼šè¯†åˆ«æ€§èƒ½ç“¶é¢ˆ

**è§£å†³äº†ä»€ä¹ˆé—®é¢˜ï¼Ÿ**

- **æˆæœ¬é€æ˜**ï¼šå®æ—¶äº†è§£APIèŠ±è´¹
- **é¢„ç®—å‘Šè­¦**ï¼šæ¥è¿‘é™é¢æ—¶è‡ªåŠ¨å‘Šè­¦
- **å†å²æ•°æ®**ï¼šæŒä¹…åŒ–å­˜å‚¨ï¼Œä¾¿äºåˆ†æ

#### ç›‘æ§æ¶æ„

```mermaid
graph TB
    subgraph "APIMonitor"
        A[APIMonitor]
        B[è®°å½•APIè°ƒç”¨]
        C[è®¡ç®—æˆæœ¬]
        D[æ£€æŸ¥é¢„ç®—]
        E[ç”ŸæˆæŠ¥å‘Š]
    end

    subgraph "LangChainå›è°ƒ"
        F[APIMonitorCallback]
        G[on_llm_start]
        H[on_llm_end]
        I[on_llm_error]
    end

    subgraph "æ•°æ®æŒä¹…åŒ–"
        J[diskcache<br/>api_usage.db]
    end

    A --> B
    A --> C
    A --> D
    A --> E

    F --> G
    F --> H
    F --> I

    H --> B
    I --> B

    A --> J
```

#### æ ¸å¿ƒå®ç°

```python
class APIMonitor:
    """
    APIä½¿ç”¨ç›‘æ§å™¨

    åŠŸèƒ½ï¼š
    1. è®°å½•æ¯æ¬¡APIè°ƒç”¨çš„tokenæ•°å’Œæˆæœ¬
    2. æ£€æŸ¥æ˜¯å¦è¶…è¿‡é¢„ç®—
    3. ç”Ÿæˆä½¿ç”¨æŠ¥å‘Š
    """

    def __init__(self, cache_dir: str = None):
        if cache_dir is None:
            cache_dir = str(Path(__file__).parent.parent.parent / "storage" / "api_monitor")

        self.cache = Cache(cache_dir)
        self.daily_budget = float(os.environ.get("API_DAILY_BUDGET", 10.0))
        self.alert_threshold = float(os.environ.get("API_ALERT_THRESHOLD", 0.8))

    def record_usage(
        self,
        model_name: str,
        prompt_tokens: int,
        completion_tokens: int,
        total_tokens: int,
        cost: float,
        metadata: Dict = None
    ):
        """è®°å½•APIä½¿ç”¨æƒ…å†µ"""
        today = datetime.now().strftime("%Y-%m-%d")
        key = f"{today}:{model_name}:{datetime.now().timestamp()}"

        record = {
            "model": model_name,
            "prompt_tokens": prompt_tokens,
            "completion_tokens": completion_tokens,
            "total_tokens": total_tokens,
            "cost": cost,
            "timestamp": datetime.now().isoformat(),
            "metadata": metadata or {}
        }

        self.cache.set(key, record)

        # æ£€æŸ¥é¢„ç®—
        daily_summary = self.get_daily_summary()
        if daily_summary["total_cost"] >= self.daily_budget * self.alert_threshold:
            logger.warning(f"APIä½¿ç”¨å‘Šè­¦ï¼šä»Šæ—¥èŠ±è´¹ {daily_summary['total_cost']:.2f} å…ƒï¼Œè¾¾åˆ°é¢„ç®—çš„ {daily_summary['total_cost']/self.daily_budget*100:.1f}%")

    def get_daily_summary(self) -> Dict:
        """è·å–ä»Šæ—¥ä½¿ç”¨æ±‡æ€»"""
        today = datetime.now().strftime("%Y-%m-%d")
        total_cost = 0.0
        total_tokens = 0
        call_count = 0

        for key in self.cache:
            if key.startswith(today):
                record = self.cache.get(key)
                total_cost += record["cost"]
                total_tokens += record["total_tokens"]
                call_count += 1

        return {
            "date": today,
            "total_cost": total_cost,
            "total_tokens": total_tokens,
            "call_count": call_count,
            "remaining_budget": self.daily_budget - total_cost
        }
```

**è®¾è®¡ç†ç”±ï¼š**

1. **ä¸ºä»€ä¹ˆç”¨diskcacheï¼Ÿ**
   - æŒä¹…åŒ–å­˜å‚¨ï¼Œç¨‹åºé‡å¯ä¸ä¸¢å¤±
   - ç®€å•æ˜“ç”¨ï¼Œç±»ä¼¼å­—å…¸API

2. **ä¸ºä»€ä¹ˆè®¾ç½®å‘Šè­¦é˜ˆå€¼ï¼Ÿ**
   - æå‰é¢„è­¦ï¼Œé¿å…è¶…é¢
   - é»˜è®¤80%ï¼Œå¯é…ç½®

3. **ä¸ºä»€ä¹ˆæŒ‰æ—¥æœŸåˆ†keyï¼Ÿ**
   - ä¾¿äºæŒ‰æ—¥ç»Ÿè®¡
   - ä¾¿äºæ¸…ç†æ—§æ•°æ®

#### LangChainå›è°ƒé›†æˆ

```python
class APIMonitorCallback(BaseCallbackHandler):
    """LangChainå›è°ƒå¤„ç†å™¨ï¼Œè‡ªåŠ¨ç›‘æ§LLMè°ƒç”¨"""

    def __init__(self, monitor: APIMonitor):
        self.monitor = monitor

    def on_llm_end(self, response: LLMResult, **kwargs):
        """LLMè°ƒç”¨ç»“æŸæ—¶è®°å½•ä½¿ç”¨æƒ…å†µ"""
        for i, res in enumerate(response.generations):
            # æå–tokenä¿¡æ¯
            token_usage = response.llm_output.get("token_usage", {})
            prompt_tokens = token_usage.get("prompt_tokens", 0)
            completion_tokens = token_usage.get("completion_tokens", 0)
            total_tokens = token_usage.get("total_tokens", 0)

            # è®¡ç®—æˆæœ¬ï¼ˆç®€åŒ–ç‰ˆï¼‰
            model_name = response.llm_output.get("model_name", "unknown")
            cost = self._calculate_cost(model_name, prompt_tokens, completion_tokens)

            # è®°å½•
            self.monitor.record_usage(
                model_name=model_name,
                prompt_tokens=prompt_tokens,
                completion_tokens=completion_tokens,
                total_tokens=total_tokens,
                cost=cost
            )
```

**è®¾è®¡ç†ç”±ï¼š**

- **ä¸ºä»€ä¹ˆç”¨å›è°ƒï¼Ÿ**
  - è‡ªåŠ¨ç›‘æ§æ‰€æœ‰LLMè°ƒç”¨
  - æ— éœ€æ‰‹åŠ¨åœ¨æ¯ä¸ªè°ƒç”¨å¤„æ·»åŠ ç›‘æ§ä»£ç 

#### æŠ€æœ¯éš¾ç‚¹

| éš¾ç‚¹ | è§£å†³æ–¹æ¡ˆ |
|------|---------|
| **æˆæœ¬è®¡ç®— | ä¸åŒæ¨¡å‹ä»·æ ¼ä¸åŒï¼Œéœ€é…ç½® |
| **æ•°æ®æŒä¹…åŒ– | diskcacheè‡ªåŠ¨æŒä¹…åŒ– |
| **å‘Šè­¦æœºåˆ¶ | æ¯æ¬¡è®°å½•åæ£€æŸ¥é˜ˆå€¼ |

---

### 3.4 åŠ¨æ€å¹¶è¡Œåº¦é…ç½®

**æ–‡ä»¶ä½ç½®ï¼š** `src/core/parallelism_config.py`

#### èƒŒæ™¯ä¸ç›®æ ‡

**ä¸ºä»€ä¹ˆéœ€è¦åŠ¨æ€å¹¶è¡Œåº¦ï¼Ÿ**

1. **CPUæ ¸å¿ƒæ•°å·®å¼‚**ï¼šä¸åŒæœºå™¨æ ¸å¿ƒæ•°ä¸åŒ
2. **ä»»åŠ¡æ•°é‡å·®å¼‚**ï¼šä»»åŠ¡å°‘æ—¶ä¸éœ€è¦å¤ªå¤šçº¿ç¨‹
3. **ä»»åŠ¡ç±»å‹å·®å¼‚**ï¼šIOå¯†é›†å‹vs CPUå¯†é›†å‹

**è§£å†³äº†ä»€ä¹ˆé—®é¢˜ï¼Ÿ**

- **èµ„æºæµªè´¹**ï¼šé¿å…åˆ›å»ºè¿‡å¤šçº¿ç¨‹
- **æ€§èƒ½ä¼˜åŒ–**ï¼šæ ¹æ®å®é™…æƒ…å†µè°ƒæ•´å¹¶è¡Œåº¦
- **å¯é…ç½®**ï¼šæ”¯æŒç¯å¢ƒå˜é‡è¦†ç›–

#### é…ç½®ç­–ç•¥

```mermaid
graph TB
    A[åŠ¨æ€å¹¶è¡Œåº¦é…ç½®] --> B{ä»»åŠ¡æ•°é‡}
    B -->|å°‘| C[ä»»åŠ¡æ•°å¹¶è¡Œ]
    B -->|å¤š| D{ä»»åŠ¡ç±»å‹}
    D -->|IOå¯†é›†| E[é«˜å¹¶è¡Œåº¦]
    D -->|CPUå¯†é›†| F[ä½å¹¶è¡Œåº¦]

    C --> G[æœ€ç»ˆå¹¶è¡Œåº¦]
    E --> G
    F --> G

    H[CPUæ ¸å¿ƒæ•°] --> I[åŸºç¡€å¹¶è¡Œåº¦]
    I --> G

    J[ç¯å¢ƒå˜é‡] --> G
```

#### æ ¸å¿ƒå®ç°

```python
class ParallelismConfig:
    """
    åŠ¨æ€å¹¶è¡Œåº¦é…ç½®

    ç­–ç•¥ï¼š
    1. åŸºäºCPUæ ¸å¿ƒæ•°è®¡ç®—åŸºç¡€å¹¶è¡Œåº¦
    2. æ ¹æ®ä»»åŠ¡æ•°é‡è°ƒæ•´
    3. æ ¹æ®ä»»åŠ¡ç±»å‹è°ƒæ•´
    4. æ”¯æŒç¯å¢ƒå˜é‡è¦†ç›–
    """

    def __init__(self):
        # CPUæ ¸å¿ƒæ•°
        self.cpu_count = os.cpu_count() or 4

        # åŸºç¡€å¹¶è¡Œåº¦ï¼ˆæ ¸å¿ƒæ•° * 2ï¼‰
        base_workers = self.cpu_count * 2

        # å…¨å±€æœ€å¤§å¹¶è¡Œåº¦ï¼ˆç¯å¢ƒå˜é‡è¦†ç›–ï¼‰
        self.max_workers = int(os.environ.get("MAX_WORKERS", base_workers))

        # ä»»åŠ¡ç‰¹å®šå¹¶è¡Œåº¦
        self.evidence_analyzer_workers = int(os.environ.get("EVIDENCE_ANALYZER_WORKERS", self.max_workers))
        self.retrieval_workers = int(os.environ.get("RETRIEVAL_WORKERS", min(self.max_workers, 12)))

        logger.info(f"å¹¶è¡Œåº¦é…ç½®: CPUæ ¸å¿ƒæ•°={self.cpu_count}, æœ€å¤§å¹¶è¡Œåº¦={self.max_workers}")

    def get_adaptive_workers(
        self,
        task_count: int,
        task_type: str = 'default',
        min_workers: int = 1
    ) -> int:
        """
        è·å–è‡ªé€‚åº”å¹¶è¡Œåº¦

        Args:
            task_count: ä»»åŠ¡æ•°é‡
            task_type: ä»»åŠ¡ç±»å‹ï¼ˆ'evidence_analyzer', 'retrieval', 'default'ï¼‰
            min_workers: æœ€å°å¹¶è¡Œåº¦

        Returns:
            è°ƒæ•´åçš„å¹¶è¡Œåº¦
        """
        # 1. åŸºäºä»»åŠ¡æ•°é‡çš„å¹¶è¡Œåº¦
        workers_by_count = min(task_count, self.max_workers)

        # 2. åŸºäºä»»åŠ¡ç±»å‹çš„å¹¶è¡Œåº¦
        if task_type == 'evidence_analyzer':
            workers_by_type = self.evidence_analyzer_workers
        elif task_type == 'retrieval':
            workers_by_type = self.retrieval_workers
        else:
            workers_by_type = self.max_workers

        # 3. å–æœ€å°å€¼
        workers = min(workers_by_count, workers_by_type)

        # 4. ç¡®ä¿ä¸å°äºæœ€å°å€¼
        workers = max(workers, min_workers)

        logger.debug(f"åŠ¨æ€å¹¶è¡Œåº¦: task_count={task_count}, task_type={task_type}, workers={workers}")
        return workers

# å•ä¾‹
_parallelism_config = None

def get_parallelism_config() -> ParallelismConfig:
    """è·å–å¹¶è¡Œåº¦é…ç½®å•ä¾‹"""
    global _parallelism_config
    if _parallelism_config is None:
        _parallelism_config = ParallelismConfig()
    return _parallelism_config
```

**è®¾è®¡ç†ç”±ï¼š**

1. **ä¸ºä»€ä¹ˆåŸºç¡€å¹¶è¡Œåº¦æ˜¯æ ¸å¿ƒæ•°*2ï¼Ÿ**
   - IOå¯†é›†å‹ä»»åŠ¡å¯ä»¥æ‰¿å—æ›´å¤šçº¿ç¨‹
   - ç»éªŒå€¼ï¼Œå¹³è¡¡æ€§èƒ½å’Œèµ„æº

2. **ä¸ºä»€ä¹ˆä»»åŠ¡ç±»å‹æœ‰ä¸åŒå¹¶è¡Œåº¦ï¼Ÿ**
   - è¯æ®åˆ†æï¼šLLMè°ƒç”¨ï¼ŒIOå¯†é›†ï¼Œå¯ä»¥é«˜å¹¶è¡Œ
   - æ£€ç´¢ï¼šå‘é‡æ£€ç´¢ï¼ŒCPU+IOæ··åˆï¼Œä¸­ç­‰å¹¶è¡Œ
   - é»˜è®¤ï¼šä½¿ç”¨æœ€å¤§å¹¶è¡Œåº¦

3. **ä¸ºä»€ä¹ˆå–æœ€å°å€¼ï¼Ÿ**
   - ä»»åŠ¡æ•°å°‘æ—¶ä¸éœ€è¦å¤ªå¤šçº¿ç¨‹
   - é¿å…èµ„æºæµªè´¹

#### ä½¿ç”¨ç¤ºä¾‹

```python
# åœ¨è¯æ®åˆ†æä¸­ä½¿ç”¨
config = get_parallelism_config()
max_workers = config.get_adaptive_workers(
    task_count=len(evidence_list),
    task_type='evidence_analyzer',
    min_workers=1
)

with concurrent.futures.ThreadPoolExecutor(max_workers=max_workers) as executor:
    # å¹¶è¡Œåˆ†æè¯æ®
    ...
```

#### æŠ€æœ¯éš¾ç‚¹

| éš¾ç‚¹ | è§£å†³æ–¹æ¡ˆ |
|------|---------|
| **èµ„æºæµªè´¹ | ä»»åŠ¡æ•°å°‘æ—¶å‡å°‘å¹¶è¡Œåº¦ |
| **ç±»å‹å·®å¼‚ | ä¸åŒä»»åŠ¡ç±»å‹ä¸åŒç­–ç•¥ |
| **å¯é…ç½® | ç¯å¢ƒå˜é‡è¦†ç›– |

---

## ç¬¬å››éƒ¨åˆ†ï¼šè®¾è®¡æ¨¡å¼ä¸æœ€ä½³å®è·µ

### 4.1 è®¾è®¡æ¨¡å¼æ€»ç»“

| è®¾è®¡æ¨¡å¼ | åº”ç”¨ä½ç½® | ç›®çš„ |
|---------|---------|------|
| **å•ä¾‹æ¨¡å¼** | RumorJudgeEngine | å…¨å±€å…±äº«ä¸€ä¸ªå¼•æ“å®ä¾‹ |
| **å·¥å‚æ¨¡å¼** | LLMFactory | ç»Ÿä¸€åˆ›å»ºä¸åŒLLMå®ä¾‹ |
| **ç­–ç•¥æ¨¡å¼** | HybridRetriever | å¯åˆ‡æ¢çš„æ£€ç´¢ç­–ç•¥ |
| **åè°ƒå™¨æ¨¡å¼** | Coordinators | åˆ†ç¦»å…³æ³¨ç‚¹ï¼Œæé«˜å¯ç»´æŠ¤æ€§ |
| **è§‚å¯Ÿè€…æ¨¡å¼** | APIMonitorCallback | è‡ªåŠ¨ç›‘æ§LLMè°ƒç”¨ |
| **æ¨¡æ¿æ–¹æ³•æ¨¡å¼** | BaseCoordinator | å®šä¹‰åè°ƒå™¨åŸºç¡€æµç¨‹ |

### 4.2 æœ€ä½³å®è·µ

#### 1. å»¶è¿Ÿåˆå§‹åŒ–

**é—®é¢˜**ï¼šé‡å‹èµ„æºï¼ˆå‘é‡åº“ã€LLMï¼‰åœ¨å¯¼å…¥æ—¶ç«‹å³åŠ è½½

**è§£å†³æ–¹æ¡ˆ**ï¼šå»¶è¿Ÿåˆ°ç¬¬ä¸€æ¬¡ä½¿ç”¨æ—¶åŠ è½½

```python
class RumorJudgeEngine:
    def __init__(self):
        self._kb = None  # å ä½ç¬¦

    @property
    def kb(self):
        if self._kb is None:
            self._kb = EvidenceKnowledgeBase()
        return self._kb
```

#### 2. ä¸Šä¸‹æ–‡ç®¡ç†å™¨

**é—®é¢˜**ï¼šé”ã€æ–‡ä»¶å¥æŸ„ç­‰èµ„æºéœ€è¦æ‰‹åŠ¨é‡Šæ”¾

**è§£å†³æ–¹æ¡ˆ**ï¼šä½¿ç”¨`with`è¯­å¥è‡ªåŠ¨é‡Šæ”¾

```python
with self._lock_mgr.acquire("component_init", timeout=30):
    # æ‰§è¡Œæ“ä½œ
    # é”è‡ªåŠ¨é‡Šæ”¾
```

#### 3. ç»†ç²’åº¦é”

**é—®é¢˜**ï¼šå¤§é”å¯¼è‡´æ€§èƒ½å·®ï¼Œå®¹æ˜“æ­»é”

**è§£å†³æ–¹æ¡ˆ**ï¼šä¸åŒèµ„æºç”¨ä¸åŒé”

```python
# å¥½ï¼šç»†ç²’åº¦é”
with self._lock_mgr.acquire("component_init"):
    # åˆå§‹åŒ–ç»„ä»¶

with self._lock_mgr.acquire("knowledge_integration"):
    # çŸ¥è¯†é›†æˆ

# åï¼šå¤§é”
with self.big_lock:
    # åˆå§‹åŒ–ç»„ä»¶
    # çŸ¥è¯†é›†æˆ
```

#### 4. ç‰ˆæœ¬æ„ŸçŸ¥

**é—®é¢˜**ï¼šç¼“å­˜ä¸çŸ¥è¯†åº“ç‰ˆæœ¬ä¸ä¸€è‡´

**è§£å†³æ–¹æ¡ˆ**ï¼šç¼“å­˜æ¡ç›®å¸¦ç‰ˆæœ¬å·

```python
# å­˜å‚¨æ—¶ç»‘å®šç‰ˆæœ¬
data["kb_version"] = self._current_kb_version.version_id

# è¯»å–æ—¶æ£€æŸ¥ç‰ˆæœ¬
if cached_data.get("kb_version") != current_version_id:
    return None  # ç‰ˆæœ¬ä¸åŒ¹é…ï¼Œç¼“å­˜å¤±æ•ˆ
```

#### 5. å¹¶è¡Œä¼˜åŒ–

**é—®é¢˜**ï¼šä¸²è¡Œå¤„ç†æ€§èƒ½å·®

**è§£å†³æ–¹æ¡ˆ**ï¼šè¯†åˆ«ç‹¬ç«‹ä»»åŠ¡ï¼Œå¹¶è¡Œæ‰§è¡Œ

```python
# æŸ¥è¯¢è§£æå’Œæœ¬åœ°æ£€ç´¢å¯ä»¥å¹¶è¡Œ
with ThreadPoolExecutor(max_workers=2) as executor:
    parse_future = executor.submit(self.parser_chain.invoke, {"query": query})
    search_future = executor.submit(self.hybrid_retriever.search_local, query)

    analysis = parse_future.result()
    local_docs = search_future.result()
```

---

## ç¬¬äº”éƒ¨åˆ†ï¼šå¼€å‘æŒ‡å—

### 5.1 ç¯å¢ƒæ­å»º

```bash
# 1. å…‹éš†ä»“åº“
git clone https://github.com/yourusername/internet_rumors_judge.git
cd internet_rumors_judge

# 2. åˆ›å»ºè™šæ‹Ÿç¯å¢ƒ
python -m venv .venv
source .venv/bin/activate  # Linux/Mac
.venv\Scripts\activate  # Windows

# 3. å®‰è£…ä¾èµ–
pip install -r requirements.txt

# 4. é…ç½®APIå¯†é’¥
export DASHSCOPE_API_KEY=your_key
export TAVILY_API_KEY=your_key  # å¯é€‰

# 5. æ„å»ºçŸ¥è¯†åº“
python -m src.retrievers.evidence_retriever build --force
```

### 5.2 å¸¸è§ä»»åŠ¡

#### æ·»åŠ æ–°çš„åˆ†æç»´åº¦

1. ä¿®æ”¹ `src/analyzers/evidence_analyzer.py`
2. åœ¨ `EvidenceAssessment` ä¸­æ·»åŠ æ–°å­—æ®µ
3. æ›´æ–°æç¤ºè¯æ¨¡æ¿
4. æ›´æ–°æµ‹è¯•ç”¨ä¾‹

#### è°ƒæ•´ç¼“å­˜ç­–ç•¥

ä¿®æ”¹ `src/config.py`:

```python
SEMANTIC_CACHE_THRESHOLD = 0.96  # è¯­ä¹‰ç¼“å­˜ç›¸ä¼¼åº¦é˜ˆå€¼
DEFAULT_CACHE_TTL = 86400  # ç¼“å­˜è¿‡æœŸæ—¶é—´ï¼ˆç§’ï¼‰
```

#### è°ƒæ•´å¹¶è¡Œåº¦

```bash
export MAX_WORKERS=20  # å…¨å±€æœ€å¤§å¹¶è¡Œåº¦
export EVIDENCE_ANALYZER_WORKERS=15  # è¯æ®åˆ†æå¹¶è¡Œåº¦
export RETRIEVAL_WORKERS=12  # æ£€ç´¢å¹¶è¡Œåº¦
```

### 5.3 è°ƒè¯•æŠ€å·§

#### å¯ç”¨è¯¦ç»†æ—¥å¿—

```python
from src.observability import configure_logging
configure_logging(log_level="DEBUG", json_output=False)
```

#### æ£€æŸ¥ç¼“å­˜å‘½ä¸­

```python
from src.core.cache_manager import CacheManager
cache = CacheManager()
result = cache.get_verdict("å–éš”å¤œæ°´ä¼šè‡´ç™Œå—ï¼Ÿ")
if result:
    print(f"ç¼“å­˜å‘½ä¸­: {result.verdict}")
```

#### æŸ¥çœ‹æ£€ç´¢ç»Ÿè®¡

```python
stats = retrieval_coordinator.get_retrieval_stats(evidence_list)
print(f"æœ¬åœ°æ£€ç´¢: {stats['local_count']}")
print(f"è”ç½‘æ£€ç´¢: {stats['web_count']}")
print(f"å»é‡å: {stats['final_count']}")
```

### 5.4 æµ‹è¯•

```bash
# è¿è¡Œæ‰€æœ‰æµ‹è¯•
pytest

# è¿è¡Œå•å…ƒæµ‹è¯•
pytest tests/unit/

# è¿è¡Œç‰¹å®šæµ‹è¯•
pytest tests/unit/test_engine.py -v

# æŸ¥çœ‹æµ‹è¯•è¦†ç›–ç‡
pytest --cov=src --cov-report=html
```

---

## é™„å½•

### A. é¡¹ç›®ç›®å½•ç»“æ„

```
internet_rumors_judge/
â”œâ”€â”€ src/                          # æºä»£ç 
â”‚   â”œâ”€â”€ core/                     # æ ¸å¿ƒå¼•æ“
â”‚   â”‚   â”œâ”€â”€ pipeline.py           # ä¸»å¼•æ“
â”‚   â”‚   â”œâ”€â”€ cache_manager.py      # ç¼“å­˜ç®¡ç†
â”‚   â”‚   â”œâ”€â”€ parallelism_config.py # å¹¶è¡Œåº¦é…ç½®
â”‚   â”‚   â”œâ”€â”€ thread_utils.py       # çº¿ç¨‹å®‰å…¨å·¥å…·
â”‚   â”‚   â”œâ”€â”€ version_manager.py    # ç‰ˆæœ¬ç®¡ç†
â”‚   â”‚   â””â”€â”€ coordinators/         # åè°ƒå™¨
â”‚   â”‚       â”œâ”€â”€ base.py           # åŸºç±»
â”‚   â”‚       â”œâ”€â”€ query_processor.py
â”‚   â”‚       â”œâ”€â”€ retrieval_coordinator.py
â”‚   â”‚       â”œâ”€â”€ analysis_coordinator.py
â”‚   â”‚       â””â”€â”€ verdict_generator.py
â”‚   â”œâ”€â”€ retrievers/               # æ£€ç´¢æ¨¡å—
â”‚   â”‚   â”œâ”€â”€ evidence_retriever.py
â”‚   â”‚   â”œâ”€â”€ hybrid_retriever.py
â”‚   â”‚   â””â”€â”€ web_search_tool.py
â”‚   â”œâ”€â”€ analyzers/                # åˆ†ææ¨¡å—
â”‚   â”‚   â”œâ”€â”€ query_parser.py
â”‚   â”‚   â”œâ”€â”€ evidence_analyzer.py
â”‚   â”‚   â””â”€â”€ truth_summarizer.py
â”‚   â”œâ”€â”€ knowledge/                # çŸ¥è¯†ç®¡ç†
â”‚   â”‚   â””â”€â”€ knowledge_integrator.py
â”‚   â”œâ”€â”€ observability/            # å¯è§‚æµ‹æ€§
â”‚   â”‚   â”œâ”€â”€ logger_config.py
â”‚   â”‚   â”œâ”€â”€ metrics.py
â”‚   â”‚   â”œâ”€â”€ api_monitor.py
â”‚   â”‚   â””â”€â”€ api_monitor_callback.py
â”‚   â”œâ”€â”€ utils/                    # å·¥å…·å‡½æ•°
â”‚   â”‚   â”œâ”€â”€ llm_factory.py
â”‚   â”‚   â”œâ”€â”€ batch_embedder.py
â”‚   â”‚   â””â”€â”€ ...
â”‚   â””â”€â”€ config.py                 # é…ç½®
â”œâ”€â”€ tests/                        # æµ‹è¯•ä»£ç 
â”‚   â”œâ”€â”€ unit/                     # å•å…ƒæµ‹è¯•
â”‚   â””â”€â”€ integration/              # é›†æˆæµ‹è¯•
â”œâ”€â”€ data/                         # æ•°æ®ç›®å½•
â”‚   â””â”€â”€ rumors/                   # è°£è¨€çŸ¥è¯†æº
â”œâ”€â”€ docs/                         # æ–‡æ¡£
â”œâ”€â”€ scripts/                      # å·¥å…·è„šæœ¬
â”œâ”€â”€ storage/                      # è¿è¡Œæ—¶æ•°æ®
â””â”€â”€ requirements.txt              # ä¾èµ–åˆ—è¡¨
```

### B. æ ¸å¿ƒé…ç½®å‚æ•°

| å‚æ•° | é»˜è®¤å€¼ | è¯´æ˜ |
|------|--------|------|
| `EMBEDDING_MODEL` | text-embedding-v4 | åµŒå…¥æ¨¡å‹ |
| `MIN_LOCAL_SIMILARITY` | 0.6 | æœ¬åœ°æ£€ç´¢ç›¸ä¼¼åº¦é˜ˆå€¼ |
| `SEMANTIC_CACHE_THRESHOLD` | 0.96 | è¯­ä¹‰ç¼“å­˜ç›¸ä¼¼åº¦é˜ˆå€¼ |
| `DEFAULT_CACHE_TTL` | 86400 | ç¼“å­˜è¿‡æœŸæ—¶é—´ï¼ˆç§’ï¼‰ |
| `AUTO_INTEGRATE_MIN_CONFIDENCE` | 90 | è‡ªåŠ¨çŸ¥è¯†é›†æˆæœ€å°ç½®ä¿¡åº¦ |
| `AUTO_INTEGRATE_MIN_EVIDENCE` | 3 | è‡ªåŠ¨çŸ¥è¯†é›†æˆæœ€å°è¯æ®æ•° |
| `MAX_WORKERS` | CPUæ ¸å¿ƒæ•°*2 | å…¨å±€æœ€å¤§å¹¶è¡Œåº¦ |
| `MODEL_PARSER` | qwen-plus | è§£æå™¨æ¨¡å‹ |
| `MODEL_ANALYZER` | qwen-plus | åˆ†æå™¨æ¨¡å‹ |
| `MODEL_SUMMARIZER` | qwen-max | è£å†³ç”Ÿæˆæ¨¡å‹ |

### C. å‚è€ƒèµ„æ–™

- [LangChainæ–‡æ¡£](https://python.langchain.com/)
- [DashScopeæ–‡æ¡£](https://dashscope.aliyun.com/)
- [ChromaDBæ–‡æ¡£](https://www.trychroma.com/)
- [é¡¹ç›®README](README.md)

---

**æ–‡æ¡£ç‰ˆæœ¬**: v1.0
**æœ€åæ›´æ–°**: 2026-02-09
**ç»´æŠ¤è€…**: Claude (å®ˆé—¨å‘˜)
