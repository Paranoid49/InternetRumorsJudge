import time
import os
import sys
import json
import datetime
from typing import List, Dict, Any
from dataclasses import dataclass, asdict
from colorama import init, Fore, Style

# åˆå§‹åŒ–æ¨¡å—
from query_parser import QueryParser
from evidence_retriever import EvidenceKnowledgeBase
from evidence_analyzer import EvidenceAnalyzer
from truth_summarizer import TruthSummarizer, VerdictType

# åˆå§‹åŒ– colorama
init(autoreset=True)

@dataclass
class TestCase:
    id: int
    query: str
    expected_entity: str
    expected_claim: str
    target_file_keyword: str
    expected_verdict: str  # "å‡" or "å¾ˆå¯èƒ½ä¸ºå‡"

@dataclass
class EvaluationResult:
    case_id: int
    parsing_score: float
    retrieval_score: float
    verdict_score: float
    total_score: float
    details: str

class RumorSystemEvaluator:
    def __init__(self):
        print(f"{Fore.CYAN}æ­£åœ¨åˆå§‹åŒ–è¯„ä¼°ç³»ç»Ÿ...{Style.RESET_ALL}")
        self.parser = QueryParser()
        self.retriever = EvidenceKnowledgeBase()
        self.analyzer = EvidenceAnalyzer()
        self.summarizer = TruthSummarizer()
        
        # 10æ¡æµ‹è¯•æ•°æ®
        self.test_cases = [
            TestCase(1, "å¬è¯´å–éš”å¤œæ°´ä¼šè‡´ç™Œ", "éš”å¤œæ°´", "è‡´ç™Œ", "11_éš”å¤œæ°´è‡´ç™Œ", "å‡"),
            TestCase(2, "åƒè”æçœŸçš„ä¼šè¢«æŸ¥å‡ºé…’é©¾å—", "è”æ", "é…’é©¾", "01_åƒè”æåå¼€è½¦ä¼šè¢«æŸ¥å‡ºé…’é©¾", "å‡"), # å®é™…ä¸Šæ˜¯å‡è°£è¨€ï¼ˆçœŸæœ‰æ­¤äº‹ï¼‰ï¼Œä½†ç»“è®ºé€šå¸¸æ˜¯â€œçœŸçš„ä½†å¾ˆå¿«æ¶ˆæ•£â€æˆ–è€…â€œéƒ¨åˆ†å±å®â€ï¼Œè¿™é‡Œæˆ‘ä»¬ä¸»è¦çœ‹ç³»ç»Ÿèƒ½å¦æ£€ç´¢åˆ°
            TestCase(3, "é…¸æ€§ä½“è´¨æ˜¯ç™¾ç—…ä¹‹æºï¼Œè¦å¤šåƒç¢±æ€§é£Ÿç‰©æŠ—ç™Œ", "é…¸æ€§ä½“è´¨", "æŠ—ç™Œ", "02_é…¸æ€§ä½“è´¨", "å‡"),
            TestCase(4, "5GåŸºç«™ä¼šä¼ æ’­æ–°å† ç—…æ¯’", "5G", "ä¼ æ’­ç—…æ¯’", "44_5Gä¼ æ’­æ–°å† ç—…æ¯’", "å‡"),
            TestCase(5, "çœ‹åˆ°åœ°éœ‡äº‘æ˜¯ä¸æ˜¯é©¬ä¸Šè¦åœ°éœ‡äº†", "åœ°éœ‡äº‘", "åœ°éœ‡", "60_åœ°éœ‡äº‘èƒ½é¢„æµ‹åœ°éœ‡", "å‡"),
            TestCase(6, "é€‚é‡å–é…’åˆ°åº•æœ‰æ²¡æœ‰ç›Šå¥åº·", "å–é…’", "æœ‰ç›Šå¥åº·", "89_é€‚é‡å–é…’æœ‰ç›Šå¥åº·", "å‡"),
            TestCase(7, "å–è›‹ç™½ç²‰ä¼šä¸ä¼šä¼¤è‚¾", "è›‹ç™½ç²‰", "ä¼¤è‚¾", "93_è›‹ç™½ç²‰ä¼¤è‚¾", "å‡"),
            TestCase(8, "æ‰‹æœºä¿¡å·å‰©ä¸€æ ¼è¾å°„ç‰¹åˆ«å¤§", "æ‰‹æœºä¿¡å·", "è¾å°„", "105_æ‰‹æœºä¿¡å·", "å‡"),
            TestCase(9, "æŠŠæ‰‹æœºå£çº¸è®¾æˆç»¿è‰²èƒ½æŠ¤çœ¼å—", "å£çº¸", "æŠ¤çœ¼", "113_çœ‹ç»¿è‰²æŠ¤çœ¼", "å‡"),
            TestCase(10, "è™¾å’Œç»´ç”Ÿç´ Cä¸€èµ·åƒä¼šä¸­æ¯’å—", "è™¾", "ä¸­æ¯’", "15_è™¾å’Œç»´ç”Ÿç´ C", "å‡")
        ]

    def evaluate_parsing(self, case: TestCase, parsed_result) -> float:
        score = 0.0
        if not parsed_result:
            return 0.0
        
        # å®ä½“åŒ¹é… (0.5åˆ†)
        if case.expected_entity in parsed_result.entity:
            score += 5.0
        
        # ä¸»å¼ åŒ¹é… (0.5åˆ†)
        if case.expected_claim in parsed_result.claim:
            score += 5.0
            
        return score

    def evaluate_retrieval(self, case: TestCase, evidences: List[Dict]) -> float:
        if not evidences:
            return 0.0
        
        # æ£€æŸ¥å‰3ä¸ªç»“æœä¸­æ˜¯å¦åŒ…å«ç›®æ ‡æ–‡ä»¶
        for i, doc in enumerate(evidences[:3]):
            source = doc.get('source', '')
            if case.target_file_keyword in source:
                # ç¬¬1å: 10åˆ†, ç¬¬2å: 8åˆ†, ç¬¬3å: 6åˆ†
                return 10.0 - (i * 2)
        
        return 0.0

    def evaluate_verdict(self, case: TestCase, verdict) -> float:
        if not verdict:
            return 0.0
        
        # ç®€åŒ–ç‰ˆé€»è¾‘ï¼šåªè¦æ˜¯ å‡ æˆ– å¾ˆå¯èƒ½ä¸ºå‡ï¼Œå°±å¾—åˆ†
        # æ³¨æ„ï¼šéƒ¨åˆ†æµ‹è¯•ç”¨ä¾‹ï¼ˆå¦‚è”æé…’é©¾ï¼‰å¯èƒ½æ˜¯â€œéƒ¨åˆ†æ”¯æŒâ€æˆ–â€œçœŸâ€ï¼Œè¿™é‡Œéœ€ç‰¹æ®Šå¤„ç†æˆ–è°ƒæ•´é¢„æœŸ
        # é‰´äºæˆ‘ä»¬çš„æ•°æ®å¤šä¸ºè°£è¨€ï¼Œé¢„æœŸå¤šä¸ºâ€œå‡â€
        
        v_str = verdict.verdict.value
        
        if v_str == "å‡":
            return 10.0
        elif v_str == "å¾ˆå¯èƒ½ä¸ºå‡":
            return 8.0
        elif v_str == "å­˜åœ¨äº‰è®®": # è§†æƒ…å†µç»™åˆ†
            return 5.0
        elif v_str == "çœŸ" and case.id == 2: # è”æé…’é©¾ç¡®å®å­˜åœ¨ç¬é—´é…’ç²¾ååº”
             return 10.0
        elif v_str == "å¾ˆå¯èƒ½ä¸ºçœŸ" and case.id == 2:
             return 8.0
             
        # å¦‚æœæ–¹å‘å®Œå…¨é”™äº†
        return 0.0

    def generate_recommendations(self, avg_scores: Dict[str, float]):
        """æ ¹æ®è¯„åˆ†ç”Ÿæˆæ”¹è¿›å»ºè®®"""
        print(f"\n{Fore.CYAN}ğŸ’¡ ä¼˜åŒ–å»ºè®®:{Style.RESET_ALL}")
        
        parsing_rate = avg_scores["parsing"]
        retrieval_rate = avg_scores["retrieval"]
        verdict_rate = avg_scores["verdict"]

        if parsing_rate < 0.8:
            print("  â€¢ ä¼˜åŒ–æŸ¥è¯¢è§£ææ™ºèƒ½ä½“çš„æç¤ºè¯ï¼Œç¡®ä¿å‡†ç¡®æå–å®ä½“å’Œä¸»å¼ ")
        
        if retrieval_rate < 0.7:
            print("  â€¢ æ£€æŸ¥å‘é‡åº“è´¨é‡ï¼Œå¯èƒ½éœ€è¦ï¼š")
            print("    - å¢åŠ æ›´å¤šæ ·åŒ–çš„è¾Ÿè°£æ•°æ®")
            print("    - è°ƒæ•´æ–‡æœ¬åˆ†å‰²ç­–ç•¥ï¼ˆchunk_sizeï¼‰")
            print("    - å°è¯•ä¸åŒçš„åµŒå…¥æ¨¡å‹")
        elif retrieval_rate < 0.9:
            print("  â€¢ æ£€ç´¢æ•ˆæœå°šå¯ï¼Œä½†ä»æœ‰æå‡ç©ºé—´ï¼Œå»ºè®®æ£€æŸ¥æ¼æ£€çš„ç‰¹å®šæ¡ˆä¾‹ï¼ˆå¦‚ï¼šå…³é”®è¯åŒ¹é…é—®é¢˜ï¼‰ã€‚")

        if verdict_rate < 0.6:
            print("  â€¢ é‡ç‚¹ä¼˜åŒ–åˆ†æä¸è£å†³æ™ºèƒ½ä½“ï¼š")
            print("    - åœ¨æç¤ºè¯ä¸­åŠ å…¥æ›´æ˜ç¡®çš„åˆ¤æ–­æ ‡å‡†")
            print("    - å¤„ç†'éƒ¨åˆ†çœŸå®'ã€'è¯æ®çŸ›ç›¾'ç­‰è¾¹ç¼˜æƒ…å†µ")
            print("    - è®©ç»“è®ºä¸è¯æ®çš„å…³è”æ›´æ˜¾å¼")
        elif verdict_rate < 0.8:
             print("  â€¢ è£å†³æ¨¡å—å¶å°”å¤±è¯¯ï¼Œå»ºè®®åˆ†æé”™è¯¯æ¡ˆä¾‹ï¼Œå¾®è°ƒPromptå¯¹â€œè¯æ®ä¸è¶³â€æˆ–â€œéƒ¨åˆ†æ”¯æŒâ€çš„åˆ¤å®šé€»è¾‘ã€‚")

        if parsing_rate >= 0.9 and retrieval_rate >= 0.9 and verdict_rate >= 0.9:
            print("  â€¢ ç³»ç»Ÿè¡¨ç°å“è¶Šï¼å¯ä»¥è€ƒè™‘å¼•å…¥æ›´å¤šè¾¹ç¼˜æµ‹è¯•ç”¨ä¾‹ï¼ˆå¦‚å¯¹æŠ—æ ·æœ¬ï¼‰è¿›è¡Œå‹åŠ›æµ‹è¯•ã€‚")

    def run(self):
        print(f"\n{Fore.GREEN}=== å¼€å§‹å…¨é¢è¯„ä¼° (å…±{len(self.test_cases)}æ¡æµ‹è¯•æ•°æ®) ==={Style.RESET_ALL}\n")
        
        results = []
        
        for case in self.test_cases:
            print(f"æ­£åœ¨è¯„ä¼° Case #{case.id}: {case.query} ... ", end="", flush=True)
            
            # 1. è§£æ
            parsed = self.parser.parse(case.query)
            p_score = self.evaluate_parsing(case, parsed)
            
            # 2. æ£€ç´¢
            evidences = []
            r_score = 0.0
            if parsed:
                # ç»„åˆæŸ¥è¯¢è¯
                query_text = f"{parsed.entity} {parsed.claim}"
                evidences = self.retriever.search(query_text, k=3)
                r_score = self.evaluate_retrieval(case, evidences)
            
            # 3. åˆ†æ & 4. è£å†³
            v_score = 0.0
            verdict_text = "N/A"
            if evidences:
                assessments = self.analyzer.analyze(parsed.claim, evidences)
                final_verdict = self.summarizer.summarize(parsed.claim, assessments)
                v_score = self.evaluate_verdict(case, final_verdict)
                if final_verdict:
                    verdict_text = final_verdict.verdict.value

            total = p_score + r_score + v_score
            
            # è®°å½•ç»“æœ
            res = EvaluationResult(
                case_id=case.id,
                parsing_score=p_score,
                retrieval_score=r_score,
                verdict_score=v_score,
                total_score=total,
                details=f"è§£æ: {parsed.entity if parsed else 'None'} | æ£€ç´¢å‘½ä¸­: {'æ˜¯' if r_score > 0 else 'å¦'} | ç»“è®º: {verdict_text}"
            )
            results.append(res)
            print(f"{Fore.GREEN}å®Œæˆ{Style.RESET_ALL} (å¾—åˆ†: {total}/30)")

        avg_scores = self.print_report(results)
        self.generate_recommendations(avg_scores)
        self.save_results(results, avg_scores)

    def print_report(self, results: List[EvaluationResult]) -> Dict[str, float]:
        print(f"\n{Fore.YELLOW}=== è¯„ä¼°æŠ¥å‘Š ==={Style.RESET_ALL}")
        print(f"{'ID':<4} {'è§£æ(10)':<10} {'æ£€ç´¢(10)':<10} {'ç»“è®º(10)':<10} {'æ€»åˆ†(30)':<10} {'è¯¦æƒ…'}")
        print("-" * 80)
        
        total_p = 0
        total_r = 0
        total_v = 0
        grand_total = 0
        
        for r in results:
            print(f"{r.case_id:<4} {r.parsing_score:<10} {r.retrieval_score:<10} {r.verdict_score:<10} {r.total_score:<10} {r.details}")
            total_p += r.parsing_score
            total_r += r.retrieval_score
            total_v += r.verdict_score
            grand_total += r.total_score
            
        avg_score = grand_total / len(results)
        
        print("-" * 80)
        print(f"å¹³å‡åˆ†: {avg_score:.2f} / 30")
        
        # åˆ†çº§
        grade = "C"
        if avg_score >= 27: grade = "S (å“è¶Š)"
        elif avg_score >= 24: grade = "A (ä¼˜ç§€)"
        elif avg_score >= 18: grade = "B (è‰¯å¥½)"
        
        print(f"ç³»ç»Ÿè¯„çº§: {Fore.RED if grade=='C' else Fore.GREEN}{grade}{Style.RESET_ALL}")
        
        # ç»´åº¦åˆ†æ
        p_rate = total_p / (len(results)*10)
        r_rate = total_r / (len(results)*10)
        v_rate = total_v / (len(results)*10)

        print(f"\nç»´åº¦å¾—åˆ†ç‡:")
        print(f"  - æŸ¥è¯¢è§£æ: {p_rate * 100:.1f}%")
        print(f"  - è¯æ®æ£€ç´¢: {r_rate * 100:.1f}%")
        print(f"  - æœ€ç»ˆè£å†³: {v_rate * 100:.1f}%")

        return {"parsing": p_rate, "retrieval": r_rate, "verdict": v_rate, "overall": avg_score}

    def save_results(self, results: List[EvaluationResult], avg_scores: Dict[str, float]):
        """ä¿å­˜è¯„ä¼°ç»“æœåˆ°JSONæ–‡ä»¶"""
        output_file = "evaluation_results.json"
        try:
            data = {
                "timestamp": datetime.datetime.now().isoformat(),
                "test_cases_count": len(results),
                "summary": {
                    "average_score": avg_scores["overall"],
                    "dimension_scores": {k: v for k, v in avg_scores.items() if k != "overall"}
                },
                "details": [asdict(r) for r in results]
            }
            
            with open(output_file, "w", encoding="utf-8") as f:
                json.dump(data, f, ensure_ascii=False, indent=2)
            
            print(f"\n{Fore.BLUE}âœ… è¯„ä¼°ç»“æœå·²ä¿å­˜è‡³: {output_file}{Style.RESET_ALL}")
        except Exception as e:
            print(f"\n{Fore.RED}âŒ ä¿å­˜ç»“æœå¤±è´¥: {e}{Style.RESET_ALL}")

if __name__ == "__main__":
    evaluator = RumorSystemEvaluator()
    evaluator.run()
