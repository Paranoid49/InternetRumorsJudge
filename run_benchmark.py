import json
import time
import logging
from typing import List, Dict
from pipeline import RumorJudgeEngine

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')
logger = logging.getLogger("Benchmark")

class BenchmarkRunner:
    def __init__(self, dataset_path: str):
        self.dataset_path = dataset_path
        self.engine = RumorJudgeEngine()
        self.results = []

    def load_dataset(self) -> List[Dict]:
        with open(self.dataset_path, 'r', encoding='utf-8') as f:
            return json.load(f)

    def run(self):
        dataset = self.load_dataset()
        total = len(dataset)
        correct = 0
        total_time = 0

        print(f"\nğŸš€ å¼€å§‹æ‰§è¡Œè‡ªåŠ¨åŒ–å›å½’æµ‹è¯• (å…± {total} æ¡)...")
        print("-" * 60)

        for i, item in enumerate(dataset, 1):
            query = item["query"]
            expected = item["expected_verdict"]
            
            print(f"[{i}/{total}] æ­£åœ¨æ ¸æŸ¥: {query}")
            
            start_time = time.time()
            try:
                # è¿è¡Œæ ¸æŸ¥å¼•æ“ (å…³é—­ç¼“å­˜ä»¥æµ‹è¯•çœŸå®æ€§èƒ½)
                res = self.engine.run(query, use_cache=False)
                duration = time.time() - start_time
                total_time += duration
                
                actual = res.final_verdict
                is_correct = (actual == expected)
                if is_correct:
                    correct += 1
                
                status_icon = "âœ…" if is_correct else "âŒ"
                print(f"   ç»“æœ: {actual} (é¢„æœŸ: {expected}) {status_icon} | è€—æ—¶: {duration:.2f}s")
                
                self.results.append({
                    "query": query,
                    "expected": expected,
                    "actual": actual,
                    "correct": is_correct,
                    "time": duration,
                    "confidence": res.confidence_score
                })
            except Exception as e:
                print(f"   âŒ å‡ºé”™: {e}")
                self.results.append({
                    "query": query,
                    "expected": expected,
                    "actual": "ERROR",
                    "correct": False,
                    "time": 0,
                    "error": str(e)
                })

        # è®¡ç®—ç»Ÿè®¡æ•°æ®
        accuracy = (correct / total) * 100 if total > 0 else 0
        avg_time = (total_time / total) if total > 0 else 0

        print("-" * 60)
        print(f"ğŸ“Š æµ‹è¯•å®ŒæˆæŠ¥å‘Š:")
        print(f"   æ€»è®¡æ•°é‡: {total}")
        print(f"   å‡†ç¡®ç‡: {accuracy:.1f}%")
        print(f"   å¹³å‡è€—æ—¶: {avg_time:.2f}s")
        print(f"   æ€»è€—æ—¶: {total_time:.2f}s")
        print("-" * 60)

        # ä¿å­˜ç»“æœåˆ°æ–‡ä»¶
        self.save_report()

    def save_report(self):
        report = {
            "timestamp": time.strftime("%Y-%m-%d %H:%M:%S"),
            "summary": {
                "total": len(self.results),
                "correct": sum(1 for r in self.results if r.get("correct")),
                "accuracy": f"{(sum(1 for r in self.results if r.get('correct')) / len(self.results)) * 100:.1f}%",
                "avg_time": f"{(sum(r.get('time', 0) for r in self.results) / len(self.results)):.2f}s"
            },
            "details": self.results
        }
        
        report_file = "benchmark_report.json"
        with open(report_file, 'w', encoding='utf-8') as f:
            json.dump(report, f, indent=2, ensure_ascii=False)
        print(f"ğŸ“ è¯¦ç»†æŠ¥å‘Šå·²ä¿å­˜è‡³: {report_file}")

if __name__ == "__main__":
    runner = BenchmarkRunner("benchmark_dataset.json")
    runner.run()
